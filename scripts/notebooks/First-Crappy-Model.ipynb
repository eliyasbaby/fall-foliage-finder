{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: GRID K520 (CNMeM is disabled, cuDNN not available)\n"
     ]
    }
   ],
   "source": [
    "# import utility libraries\n",
    "from netCDF4 import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.basemap import Basemap, cm\n",
    "import cv2\n",
    "%matplotlib inline\n",
    "\n",
    "# import machine learning tools\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential, Graph\n",
    "from keras.layers import Dense, Flatten, Activation, Dropout\n",
    "from keras.layers.convolutional import Convolution2D, ZeroPadding2D, MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "from keras import backend as K\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "from clustering import Location_Clusterer\n",
    "from util import reformat_y, plot_list_in_2D\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "folder = '/home/ubuntu/dataset/'\n",
    "files = ['veg.nc', 'ppt.monthly.mask.nc', 'tmean.monthly.mask.nc', 'elev.nc']\n",
    "var_names = ['Cv', 'Band1', 'Band1', 'elev']\n",
    "\n",
    "lc = Location_Clusterer(n_clusters=n)\n",
    "for f, var in zip(files, var_names):\n",
    "    lc.read_data(folder+f, var)\n",
    "\n",
    "lc.transform_data()\n",
    "clusters = lc.fit_predict(lc.data2d)\n",
    "\n",
    "#plot_list_in_2D(lc.coords2d[:,0], lc.coords2d[:,1], clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# for i in xrange(n):\n",
    "#     ind = (clusters==i)\n",
    "#     plot_list_in_2D(lc.coords2d[:,0][ind], lc.coords2d[:,1][ind], clusters[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting nn_input.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile nn_input.py\n",
    "# import utility libraries\n",
    "from netCDF4 import Dataset\n",
    "import numpy as np\n",
    "\n",
    "class NN_Input(object):\n",
    "    \"\"\"\n",
    "    Stores the input data ready for feeding into a Keras neural network. \n",
    "\n",
    "    To-Do:\n",
    "    - add function to take the clustering data in some ways\n",
    "    - add function to return the actual lat, lon, and time based on indices\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, predict=2, history=2, box=5, random_seed=None, fill_value=-999):\n",
    "        \"\"\"\n",
    "        Initialize a class for storing neural network input data. \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        predict: int, number of time points ahead that the model will predict. \n",
    "                 For example, if predict=2, the model will predict 2 time points away from the given time. \n",
    "        history: int, number of time points for which data would be included as input.\n",
    "                 For example, if data_length=3, the model will receive 3 time points worth of data (current time\n",
    "                 point, the previous time point, and the timep point before that).\n",
    "        \"\"\"\n",
    "        if random_seed is not None:\n",
    "            np.random.seed(random_seed)\n",
    "        self.lons = None\n",
    "        self.lats = None\n",
    "        self.times = None\n",
    "        \n",
    "        self.labels = None\n",
    "        self.features = {}\n",
    "        self.feature_types = {}\n",
    "        self.variables = []\n",
    "        self.fill_value = fill_value\n",
    "        \n",
    "        self.predict = predict\n",
    "        self.history = history\n",
    "        self.box = box\n",
    "        \n",
    "    def load_labels(self, f_path, var):\n",
    "        \"\"\"\n",
    "        Load labels from netCDF file. \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        f_path: string\n",
    "        var: string\n",
    "        \"\"\"\n",
    "        nc = Dataset(f_path, 'r')\n",
    "        self.lons = nc.variables['lon'][:]\n",
    "        self.lats = nc.variables['lat'][:]\n",
    "        \n",
    "        self.times = nc.variables['time'][self.history:-self.predict]\n",
    "        n = self.predict + self.history\n",
    "        self.labels = nc.variables[var][n:,:,:]\n",
    "        \n",
    "    def load_features(self, f_path, var, name, feature_type):\n",
    "        \"\"\"\n",
    "        Load feature values from netCDF files. Stores feature type information. \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        f_path: string, path to input netCDF file.\n",
    "        var: string, variable name as appeared in the netCDF file. \n",
    "        name: string, name of the variable to be stored. \n",
    "        feature_type: string, must be one of the following: 'history_time_series', 'forecast_time_series', \n",
    "        'multi_layers', 'single_layer'\n",
    "        \"\"\"\n",
    "        nc = Dataset(f_path, 'r')\n",
    "        temp_data = nc.variables[var][:]\n",
    "        \n",
    "        # Storing information on whether the input features \n",
    "        self.feature_types[name] = feature_type\n",
    "        self.variables.append(name)\n",
    "        \n",
    "        if self.feature_types[name] == 'history_time_series':\n",
    "            self.features[name] = temp_data[:-self.predict, :, :]\n",
    "        elif self.feature_types[name] == 'forecast_time_series':\n",
    "            self.features[name] = temp_data[self.history:, :, :]\n",
    "        else:\n",
    "            self.features[name] = temp_data\n",
    "        \n",
    "    def get_batch(self, j, k):\n",
    "        pass\n",
    "    \n",
    "    def get_features(self, i, j, k, ndim_out):\n",
    "        \"\"\"\n",
    "        Given indices for latitude, longitude, and time point, returns the associated data from self.data. \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        lat: int, index for the latitude desired. Must be within the range available in self.data. \n",
    "        lon: int, index for the longitude desired. Must be within the range available in self.data. \n",
    "        time: int, index for the time point desired. Must be within the range available in self.data. \n",
    "        \"\"\"\n",
    "        maps = None\n",
    "#         lst = None\n",
    "        for ix, feat in enumerate(self.variables):\n",
    "            if self.feature_types[feat] == 'history_time_series':\n",
    "                temp_data = self.features[feat][i:i+self.history+1, j-self.box:j+self.box+1, k-self.box:k+self.box+1]\n",
    "            elif self.feature_types[feat] == 'forecast_time_series':\n",
    "                temp_data = self.features[feat][i:i+self.predict+1, j-self.box:j+self.box+1, k-self.box:k+self.box+1]\n",
    "#             elif self.feature_types[feat] == 'multi_layers':\n",
    "#                 temp_data = self.features[feat][:, j, k].flatten()\n",
    "#             else: \n",
    "#                 temp_data = self.features[feat][j, k]\n",
    "            \n",
    "            if len(temp_data.shape) == 3:                \n",
    "                if np.sum(temp_data.mask) > len(temp_data.flatten())/2:\n",
    "                    return None\n",
    "                elif np.any(temp_data.mask):\n",
    "                    temp_data = temp_data.filled(self.fill_value)\n",
    "                    \n",
    "                if maps is None:\n",
    "                    maps = temp_data\n",
    "                else:\n",
    "                    if ndim_out == 4:\n",
    "                        maps = np.ma.concatenate((maps, temp_data), axis=0)\n",
    "                    elif ndim_out == 5:\n",
    "                        maps = np.stack((maps, temp_data))\n",
    "#             else:\n",
    "#                 if lst is None:\n",
    "#                     lst = temp_data\n",
    "#                 else:\n",
    "#                     lst = np.append(lst, temp_data)\n",
    "#         return [maps, lst]\n",
    "        return maps\n",
    "        \n",
    "    def select(self, n=None, t=None, cutoff=None, subset=None, ndim_out=4):\n",
    "        \"\"\"\n",
    "        Selecting n data points randomly from the database before specified time cutoff. \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        n: int, number of data points wanted. \n",
    "        cutoff: int, time cutoff for the training dataset. Default is half of the data available. \n",
    "        \"\"\"\n",
    "        if cutoff is None:\n",
    "            cutoff = len(self.times)/2\n",
    "        \n",
    "        indices, labels, output_maps, output_lst = [], [], [], []\n",
    "        \n",
    "        mi = 0\n",
    "        for ix, feat in enumerate(self.variables):\n",
    "            if self.feature_types[feat] == 'history_time_series':\n",
    "                mi += (self.history+1)\n",
    "            elif self.feature_types[feat] == 'forecast_time_series':\n",
    "                mi += (self.predict+1)\n",
    "                \n",
    "        if ndim_out == 4:\n",
    "            map_dimensions = (mi, (2*self.box)+1, (2*self.box)+1)\n",
    "        elif ndim_out == 5:\n",
    "            map_dimensions = (len(self.variables), self.history+1, (2*self.box)+1, (2*self.box)+1)\n",
    "        \n",
    "        if n is None and t is None:\n",
    "            for i in xrange(cutoff):\n",
    "                for (k, j) in subset:\n",
    "                    l = self.labels[i, j, k]\n",
    "                    features = self.get_features(i, j, k, ndim_out=ndim_out)\n",
    "                    if features is not None and l != np.nan and features.shape==map_dimensions:\n",
    "                        indices.append([i, j, k])\n",
    "                        labels.append(l)\n",
    "                        output_maps.append(features)\n",
    "#                         output_lst.append(features[1])\n",
    "        \n",
    "        elif t is not None:\n",
    "            for (k, j) in subset:\n",
    "                l = self.labels[t, j, k]\n",
    "                features = self.get_features(t, j, k, ndim_out=ndim_out)\n",
    "                if features is not None and l != np.nan and features.shape==map_dimensions:\n",
    "                    indices.append([t, j, k])\n",
    "                    labels.append(l)\n",
    "                    output_maps.append(features)\n",
    "#                     output_lst.append(features[1])\n",
    "        \n",
    "        else:\n",
    "            while len(labels) < n:\n",
    "                if subset is not None:\n",
    "                    (k, j) = subset[np.random.choice(len(subset))]\n",
    "                else:\n",
    "                    j = np.random.randint(self.box, len(self.lats)-self.box)\n",
    "                    k = np.random.randint(self.box, len(self.lons)-self.box)\n",
    "                if cutoff < 0:\n",
    "                    i = np.random.randint(cutoff, 0)\n",
    "                else:\n",
    "                    i = np.random.randint(cutoff)\n",
    "                l = self.labels[i, j, k]\n",
    "                features = self.get_features(i, j, k, ndim_out=ndim_out)\n",
    "                if features is not None and l != np.nan and features.shape==map_dimensions:\n",
    "                    indices.append([i, j, k])\n",
    "                    labels.append(l)\n",
    "                    output_maps.append(features)\n",
    "#                     output_lst.append(features[1])\n",
    "                    \n",
    "        return np.array(indices), np.array(labels), np.array(output_maps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nn_input import NN_Input\n",
    "\n",
    "nn = NN_Input(predict=2, history=5, box=5, random_seed=42)\n",
    "nn.load_labels(folder+'sign.label.nc', 'Band1')\n",
    "\n",
    "# f_paths = ['all.ndvi.nc','all.max.of.Wind.nc', 'all.min.of.Tmin.nc', 'all.mean.of.Tmin.nc', 'all.sum.of.Prec.nc',\n",
    "#            'all.max.of.Tmax.nc', 'all.mean.of.Tmax.nc','elev.nc', 'veg.nc']\n",
    "# variables = ['Band1', 'Wind', 'Tmin', 'Tmin', 'Prec', 'Tmax', 'Tmax', 'elev', 'Cv']\n",
    "# names = ['ndvi', 'max_wind', 'min_tmin', 'mean_tmin', 'total_prec', 'max_tmax', 'mean_tmax', 'elev', 'veg']\n",
    "# feature_types = ['history_time_series', 'forecast_time_series', 'forecast_time_series', 'forecast_time_series',\n",
    "#                  'forecast_time_series', 'forecast_time_series', 'forecast_time_series',\n",
    "#                 'single_layer', 'multi_layers']\n",
    "\n",
    "f_paths = ['all.mean.of.Tmin.nc', 'all.mean.of.Tmin.nc']\n",
    "variables = ['Tmin', 'Tmin']\n",
    "names = ['mean_tmin_history', 'mean_tmin_forecast']\n",
    "feature_types = ['history_time_series', 'forecast_time_series']\n",
    "\n",
    "for f_path, v, n, feature_type in zip(f_paths, variables, names, feature_types):\n",
    "    nn.load_features(folder+f_path, v, n, feature_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modeling for cluster 1\n",
      "Getting training dataset\n",
      "(9, 11, 11)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-5ac8d3a1d667>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m'Getting training dataset'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mid_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_map_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreformat_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ubuntu/fall-foliage-finder/scripts/nn_input.py\u001b[0m in \u001b[0;36mselect\u001b[1;34m(self, n, t, cutoff, subset, ndim_out)\u001b[0m\n\u001b[0;32m    190\u001b[0m                 \u001b[0ml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m                 \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mndim_out\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mndim_out\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 192\u001b[1;33m                 \u001b[1;32mprint\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    193\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0ml\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnan\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mmap_dimensions\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m                     \u001b[0mindices\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "point = [308, 290]\n",
    "for i, loc in enumerate(lc.ind2d):\n",
    "    if loc[0] == point[0] and loc[1] == point[1]:\n",
    "        cluster = clusters[i]\n",
    "\n",
    "print 'Modeling for cluster', cluster\n",
    "subset = lc.ind2d[clusters==cluster]\n",
    "\n",
    "print 'Getting training dataset'\n",
    "id_train, y_train, X_map_train = nn.select(n=100000, subset=subset)\n",
    "y_train = reformat_y(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'Getting test dataset'\n",
    "\n",
    "t = (len(nn.times)/2) +1\n",
    "print len(subset)\n",
    "id_test, y_test, X_map_test = nn.select(t=t, subset=subset)\n",
    "y_test = reformat_y(y_test)\n",
    "print len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "map_dimensions=X_map_train[0].shape\n",
    "print map_dimensions\n",
    "print np.all(X_map_train[0] == X_map_train[1])\n",
    "\n",
    "mean_train = np.mean(X_map_train.flatten())\n",
    "std_train = np.std(X_map_train.flatten())\n",
    "X_map_train = (X_map_train-mean_train)/std_train\n",
    "X_map_test = (X_map_test-mean_train)/std_train\n",
    "print y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_base_sequential_NN(n_conv_layers=2, nb_filters=16, nb_conv=3, map_dimensions=None, nb_pool=2):\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(nb_filters, nb_conv, nb_conv,\n",
    "                            border_mode='valid',\n",
    "                            input_shape=map_dimensions))\n",
    "    \n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Convolution2D(nb_filters, nb_conv, nb_conv))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(nb_pool, nb_pool)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(2))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    sgd = SGD()\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=sgd,\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = build_base_sequential_NN(nb_filters=64, map_dimensions=map_dimensions)\n",
    "model.fit(X_map_train, y_train, batch_size=50, nb_epoch=5, verbose=True, validation_data=(X_map_test, y_test))\n",
    "train_predict = model.predict(X_map_train, verbose=True)\n",
    "test_predict = model.predict(X_map_test, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "image_paths = []\n",
    "for t in xrange(216, 221):\n",
    "\n",
    "    id_test, y_test, X_map_test = nn.select(t=t, subset=subset)\n",
    "    y_test = reformat_y(y_test)\n",
    "    X_map_test = (X_map_test-mean_train)/std_train\n",
    "    \n",
    "    timestamp = nn.times[(id_test[0,0]).astype(int)]\n",
    "    xs = nn.lons[(id_test[:,2]).astype(int)]\n",
    "    ys = nn.lats[(id_test[:,1]).astype(int)]\n",
    "    test_predict = model.predict(X_map_test)\n",
    "    \n",
    "    path = plot_compare_map(xs, ys, y_test[:,0], (test_predict[:,0]>threshold), \n",
    "                            timestamp, folder='/home/ubuntu/dataset/output/')\n",
    "    image_paths.append(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "animate_maps(image_paths, '/home/ubuntu/dataset/output/test.mp4')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
