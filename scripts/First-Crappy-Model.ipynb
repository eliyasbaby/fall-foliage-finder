{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: GRID K520 (CNMeM is disabled, cuDNN not available)\n"
     ]
    }
   ],
   "source": [
    "# import utility libraries\n",
    "from netCDF4 import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.basemap import Basemap, cm\n",
    "%matplotlib inline\n",
    "\n",
    "# import machine learning tools\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential, Graph\n",
    "from keras.layers import Dense, Flatten, Activation, Dropout\n",
    "from keras.layers.convolutional import Convolution2D, ZeroPadding2D, MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "from keras import backend as K\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "from clustering import Location_Clusterer, plot_list_in_2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "folder = '/home/ubuntu/dataset/'\n",
    "files = ['veg.nc', 'ppt.monthly.mask.nc', 'tmean.monthly.mask.nc', 'elev.nc']\n",
    "var_names = ['Cv', 'Band1', 'Band1', 'elev']\n",
    "\n",
    "lc = Location_Clusterer(n_clusters=n)\n",
    "for f, var in zip(files, var_names):\n",
    "    lc.read_data(folder+f, var)\n",
    "\n",
    "lc.transform_data()\n",
    "clusters = lc.fit_predict(lc.data2d)\n",
    "\n",
    "#plot_list_in_2D(lc.coords2d[:,0], lc.coords2d[:,1], clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# for i in xrange(n):\n",
    "#     ind = (clusters==i)\n",
    "#     plot_list_in_2D(lc.coords2d[:,0][ind], lc.coords2d[:,1][ind], clusters[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting nn_input.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile nn_input.py\n",
    "# import utility libraries\n",
    "from netCDF4 import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "\n",
    "# import machine learning tools\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Convolution2D, ZeroPadding2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "\n",
    "# import utilities and classes I wrote\n",
    "from clustering import Location_Clusterer\n",
    "\n",
    "class NN_Input(object):\n",
    "    \"\"\"\n",
    "    Stores the input data ready for feeding into a Keras neural network. \n",
    "\n",
    "    To-Do:\n",
    "    - add function to take the clustering data in some ways\n",
    "    - add function to return the actual lat, lon, and time based on indices\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, predict=2, history=2, box=5):\n",
    "        \"\"\"\n",
    "        Initialize a class for storing neural network input data. \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        predict: int, number of time points ahead that the model will predict. \n",
    "                 For example, if predict=2, the model will predict 2 time points away from the given time. \n",
    "        history: int, number of time points for which data would be included as input.\n",
    "                 For example, if data_length=3, the model will receive 3 time points worth of data (current time\n",
    "                 point, the previous time point, and the timep point before that).\n",
    "        \"\"\"\n",
    "        self.lons = None\n",
    "        self.lats = None\n",
    "        self.times = None\n",
    "        \n",
    "        self.labels = None\n",
    "        self.features = {}\n",
    "        self.feature_types = {}\n",
    "        self.variables = []\n",
    "        \n",
    "        self.predict = predict\n",
    "        self.history = history\n",
    "        self.box = box\n",
    "        \n",
    "    def load_labels(self, f_path, var):\n",
    "        \"\"\"\n",
    "        Load labels from netCDF file. \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        f_path: string\n",
    "        var: string\n",
    "        \"\"\"\n",
    "        nc = Dataset(f_path, 'r')\n",
    "        self.lons = nc.variables['lon'][:]\n",
    "        self.lats = nc.variables['lat'][:]\n",
    "        \n",
    "        self.times = nc.variables['time'][self.history:-self.predict]\n",
    "        n = self.predict + self.history\n",
    "        self.labels = nc.variables[var][n:,:,:]\n",
    "        \n",
    "    def load_features(self, f_path, var, name, feature_type):\n",
    "        \"\"\"\n",
    "        Load feature values from netCDF files. Stores feature type information. \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        f_path: string, path to input netCDF file.\n",
    "        var: string, variable name as appeared in the netCDF file. \n",
    "        name: string, name of the variable to be stored. \n",
    "        feature_type: string, must be one of the following: 'history_time_series', 'forecast_time_series', \n",
    "        'multi_layers', 'single_layer'\n",
    "        \"\"\"\n",
    "        nc = Dataset(f_path, 'r')\n",
    "        temp_data = nc.variables[var][:]\n",
    "        \n",
    "        # Storing information on whether the input features \n",
    "        self.feature_types[name] = feature_type\n",
    "        self.variables.append(name)\n",
    "        \n",
    "        if self.feature_types[name] == 'history_time_series':\n",
    "            self.features[name] = temp_data[:-self.predict, :, :]\n",
    "        elif self.feature_types[name] == 'forecast_time_series':\n",
    "            self.features[name] = temp_data[self.history:, :, :]\n",
    "        else:\n",
    "            self.features[name] = temp_data\n",
    "        \n",
    "    \n",
    "    def get_features(self, i, j, k):\n",
    "        \"\"\"\n",
    "        Given indices for latitude, longitude, and time point, returns the associated data from self.data. \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        lat: int, index for the latitude desired. Must be within the range available in self.data. \n",
    "        lon: int, index for the longitude desired. Must be within the range available in self.data. \n",
    "        time: int, index for the time point desired. Must be within the range available in self.data. \n",
    "        \"\"\"\n",
    "        maps = None\n",
    "#         lst = None\n",
    "        for ix, feat in enumerate(self.variables):\n",
    "            if self.feature_types[feat] == 'history_time_series':\n",
    "                temp_data = self.features[feat][i:i+self.history+1, j-self.box:j+self.box+1, k-self.box:k+self.box+1]\n",
    "            elif self.feature_types[feat] == 'forecast_time_series':\n",
    "                temp_data = self.features[feat][i:i+self.predict+1, j-self.box:j+self.box+1, k-self.box:k+self.box+1]\n",
    "#             elif self.feature_types[feat] == 'multi_layers':\n",
    "#                 temp_data = self.features[feat][:, j, k].flatten()\n",
    "#             else: \n",
    "#                 temp_data = self.features[feat][j, k]\n",
    "            \n",
    "            if len(temp_data.shape) == 3:                \n",
    "                if np.sum(temp_data.mask) > len(temp_data.flatten())/2:\n",
    "                    return None\n",
    "                elif np.any(temp_data.mask):\n",
    "                    temp_data = temp_data.filled(-999)\n",
    "                    \n",
    "                if maps is None:\n",
    "                    maps = temp_data\n",
    "                else:\n",
    "                    maps = np.ma.concatenate((maps, temp_data), axis=0)\n",
    "#             else:\n",
    "#                 if lst is None:\n",
    "#                     lst = temp_data\n",
    "#                 else:\n",
    "#                     lst = np.append(lst, temp_data)\n",
    "#         return [maps, lst]\n",
    "        return maps\n",
    "        \n",
    "    def select(self, n=None, t=None, cutoff=None, subset=None):\n",
    "        \"\"\"\n",
    "        Selecting n data points randomly from the database before specified time cutoff. \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        n: int, number of data points wanted. \n",
    "        cutoff: int, time cutoff for the training dataset. Default is half of the data available. \n",
    "        \"\"\"\n",
    "        if cutoff is None:\n",
    "            cutoff = len(self.times)/2\n",
    "        \n",
    "        indices, labels, output_maps, output_lst = [], [], [], []\n",
    "        \n",
    "        mi = 0\n",
    "        for ix, feat in enumerate(self.variables):\n",
    "            if self.feature_types[feat] == 'history_time_series':\n",
    "                mi += (self.history+1)\n",
    "            elif self.feature_types[feat] == 'forecast_time_series':\n",
    "                mi += (self.predict+1)\n",
    "                \n",
    "        map_dimensions = (mi, (2*self.box)+1, (2*self.box)+1)\n",
    "\n",
    "        if n is None:\n",
    "            for (k, j) in subset:\n",
    "                for i in xrange(cutoff):\n",
    "                    l = self.labels[i, j, k]\n",
    "                    features = self.get_features(i, j, k)\n",
    "                    if features is not None and l != np.nan and features.shape==map_dimensions:\n",
    "                        indices.append([i, j, k])\n",
    "                        labels.append(l)\n",
    "                        output_maps.append(features)\n",
    "#                         output_lst.append(features[1])\n",
    "        \n",
    "        elif t is not None:\n",
    "            for (k, j) in subset:\n",
    "                l = self.labels[t, j, k]\n",
    "                features = self.get_features(t, j, k)\n",
    "                if features is not None and l != np.nan and features.shape==map_dimensions:\n",
    "                    indices.append([t, j, k])\n",
    "                    labels.append(l)\n",
    "                    output_maps.append(features)\n",
    "#                     output_lst.append(features[1])\n",
    "        \n",
    "        else:\n",
    "            while len(labels) < n:\n",
    "                if subset is not None:\n",
    "                    (k, j) = subset[np.random.choice(len(subset))]\n",
    "                else:\n",
    "                    j = np.random.randint(self.box, len(self.lats)-self.box)\n",
    "                    k = np.random.randint(self.box, len(self.lons)-self.box)\n",
    "                i = np.random.randint(cutoff)\n",
    "                l = self.labels[i, j, k]\n",
    "                features = self.get_features(i, j, k)\n",
    "                if features is not None and l != np.nan and features.shape==map_dimensions:\n",
    "                    indices.append([i, j, k])\n",
    "                    labels.append(l)\n",
    "                    output_maps.append(features)\n",
    "#                     output_lst.append(features[1])\n",
    "                    \n",
    "        return np.array(indices), np.array(labels), np.array(output_maps)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reformat_y(y):\n",
    "    y[y == -1] = 0\n",
    "    y = np.hstack((y.reshape(-1,1), 1-y.reshape(-1,1)))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nn_input import NN_Input\n",
    "\n",
    "nn = NN_Input(predict=2, history=5, box=5)\n",
    "nn.load_labels(folder+'sign.label.nc', 'Band1')\n",
    "\n",
    "# f_paths = ['all.ndvi.nc','all.max.of.Wind.nc', 'all.min.of.Tmin.nc', 'all.mean.of.Tmin.nc', 'all.sum.of.Prec.nc',\n",
    "#            'all.max.of.Tmax.nc', 'all.mean.of.Tmax.nc','elev.nc', 'veg.nc']\n",
    "# variables = ['Band1', 'Wind', 'Tmin', 'Tmin', 'Prec', 'Tmax', 'Tmax', 'elev', 'Cv']\n",
    "# names = ['ndvi', 'max_wind', 'min_tmin', 'mean_tmin', 'total_prec', 'max_tmax', 'mean_tmax', 'elev', 'veg']\n",
    "# feature_types = ['history_time_series', 'forecast_time_series', 'forecast_time_series', 'forecast_time_series',\n",
    "#                  'forecast_time_series', 'forecast_time_series', 'forecast_time_series',\n",
    "#                 'single_layer', 'multi_layers']\n",
    "\n",
    "f_paths = ['all.mean.of.Tmin.nc', 'all.mean.of.Tmin.nc']\n",
    "variables = ['Tmin', 'Tmin']\n",
    "names = ['mean_tmin_history', 'mean_tmin_forecast']\n",
    "feature_types = ['history_time_series', 'forecast_time_series']\n",
    "\n",
    "for f_path, v, n, feature_type in zip(f_paths, variables, names, feature_types):\n",
    "    nn.load_features(folder+f_path, v, n, feature_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modeling for cluster 5\n",
      "Getting training dataset\n"
     ]
    }
   ],
   "source": [
    "point = [308, 290]\n",
    "for i, loc in enumerate(lc.ind2d):\n",
    "    if loc[0] == point[0] and loc[1] == point[1]:\n",
    "        cluster = clusters[i]\n",
    "\n",
    "print 'Modeling for cluster', cluster\n",
    "subset = lc.ind2d[clusters==cluster]\n",
    "\n",
    "print 'Getting training dataset'\n",
    "id_train, y_train, X_map_train = nn.select(n=100000, subset=subset)\n",
    "y_train = reformat_y(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting test dataset\n",
      "216\n",
      "17869\n",
      "196\n"
     ]
    }
   ],
   "source": [
    "print 'Getting test dataset'\n",
    "\n",
    "t = (len(nn.times)/2) +1\n",
    "print t\n",
    "print len(subset)\n",
    "id_test, y_test, X_map_test = nn.select(t=t, subset=subset)\n",
    "y_test = reformat_y(y_test)\n",
    "print len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 11, 11)\n",
      "False\n",
      "(100000, 2)\n"
     ]
    }
   ],
   "source": [
    "map_dimensions=X_map_train[0].shape\n",
    "print map_dimensions\n",
    "print np.all(X_map_train[0] == X_map_train[1])\n",
    "\n",
    "std = np.max(X_map_train)\n",
    "X_map_train = X_map_train/std\n",
    "X_map_test = X_map_test/std\n",
    "print y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "100000/100000 [==============================] - 15s - loss: 0.8262 - acc: 0.7993    \n",
      "Epoch 2/5\n",
      "100000/100000 [==============================] - 15s - loss: 0.3806 - acc: 0.8521    \n",
      "Epoch 3/5\n",
      "100000/100000 [==============================] - 15s - loss: 0.3296 - acc: 0.8765    \n",
      "Epoch 4/5\n",
      "100000/100000 [==============================] - 15s - loss: 0.2923 - acc: 0.8969    \n",
      "Epoch 5/5\n",
      "100000/100000 [==============================] - 15s - loss: 0.2665 - acc: 0.9086    \n",
      "100000/100000 [==============================] - 9s     \n",
      " 587072/3502972 [====>.........................] - ETA: 290s"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-efbbc2bab27c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_map_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[0mtrain_predict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_map_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m \u001b[0mtest_predict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_map_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/ubuntu/anaconda/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose)\u001b[0m\n\u001b[0;32m    451\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    452\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'The model needs to be compiled before being used.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 453\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    454\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    455\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ubuntu/anaconda/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose)\u001b[0m\n\u001b[0;32m   1101\u001b[0m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m         return self._predict_loop(f, ins,\n\u001b[1;32m-> 1103\u001b[1;33m                                   batch_size=batch_size, verbose=verbose)\n\u001b[0m\u001b[0;32m   1104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1105\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[1;32m/home/ubuntu/anaconda/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_predict_loop\u001b[1;34m(self, f, ins, batch_size, verbose)\u001b[0m\n\u001b[0;32m    832\u001b[0m                 \u001b[0mouts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch_start\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbatch_end\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_out\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    833\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mverbose\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 834\u001b[1;33m                 \u001b[0mprogbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_end\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    835\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    836\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ubuntu/anaconda/lib/python2.7/site-packages/keras/utils/generic_utils.pyc\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, current, values)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m             \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 113\u001b[1;33m             \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcurrent\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ubuntu/anaconda/lib/python2.7/site-packages/ipykernel/iostream.pyc\u001b[0m in \u001b[0;36mflush\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    266\u001b[0m             \u001b[0mevt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mthreading\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEvent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_io_loop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_callback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 268\u001b[1;33m             \u001b[0mevt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    269\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ubuntu/anaconda/lib/python2.7/threading.pyc\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    612\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__cond\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    613\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__flag\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 614\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__cond\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    615\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__flag\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ubuntu/anaconda/lib/python2.7/threading.pyc\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    338\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 340\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    341\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0m__debug__\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_note\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%s.wait(): got it\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def build_base_sequential_NN(n_conv_layers=2, nb_filters=16, nb_conv=3, map_dimensions=None, nb_pool=2):\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(nb_filters, nb_conv, nb_conv,\n",
    "                            border_mode='valid',\n",
    "                            input_shape=map_dimensions))\n",
    "    \n",
    "    model.add(Activation('relu'))\n",
    "#     model.add(Convolution2D(nb_filters, nb_conv, nb_conv))\n",
    "#     model.add(Activation('relu'))\n",
    "#     model.add(MaxPooling2D(pool_size=(nb_pool, nb_pool)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(2))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    sgd = SGD()\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=sgd,\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = build_base_sequential_NN(nb_filters=64, map_dimensions=map_dimensions)\n",
    "model.fit(X_map_train, y_train, batch_size=50, nb_epoch=5, verbose=True)\n",
    "train_predict = model.predict(X_map_train, verbose=True)\n",
    "test_predict = model.predict(X_map_test, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Training set:\n",
      "Bench mark: 0.65512\n",
      "Accuracy: 0.91728\n",
      "ROC AUC: 0.951746278202\n",
      "\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.5\n",
    "\n",
    "print 'Training set:'\n",
    "print 'Bench mark:', np.sum(y_train[:,1])/float(len(y_train))\n",
    "print 'Accuracy:', accuracy_score(y_train[:,0], (train_predict[:,0]>threshold))\n",
    "print 'ROC AUC:', roc_auc_score(y_train[:,0], train_predict[:,0])\n",
    "print \n",
    "print 'Testing set:'\n",
    "print 'Bench mark:', np.sum(test_y[:,1])/float(len(test_y))\n",
    "print 'Accuracy:', accuracy_score(y_test[:,0], (test_predict[:,0]>threshold))\n",
    "print 'ROC AUC:', roc_auc_score(y_test[:,0], test_predict[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # # graph model with two inputs and one output \n",
    "# # graph = Graph() \n",
    "# # graph.add_input(name='input1', input_shape=(32,)) \n",
    "# # graph.add_input(name='input2', input_shape=(32,)) \n",
    "# # graph.add_node(Dense(16), name='dense1', input='input1') \n",
    "# # graph.add_node(Dense(4), name='dense2', input='input2') \n",
    "# # graph.add_node(Dense(4), name='dense3', input='dense1') \n",
    "\n",
    "# # graph.add_output(name='output', inputs=['dense2', 'dense3'], merge_mode='sum') \n",
    "\n",
    "# # graph.compile(optimizer='rmsprop', loss={'output':'mse'}) \n",
    "# # history = graph.fit({'input1':X_train, 'input2':X2_train, 'output':y_train}, nb_epoch=10) \n",
    "# # predictions = graph.predict({'input1':X_test, 'input2':X2_test}) # {'output':...}\n",
    "\n",
    "\n",
    "# # graph model with two inputs and one output \n",
    "# model = Graph() \n",
    "\n",
    "# map_dimensions=train_X_map[0][0].shape\n",
    "\n",
    "# # two types of inputs: maps in 3D matrix and a list\n",
    "# model.add_input(name='maps', input_shape=train_X_map[0].shape) \n",
    "# model.add_input(name='lst', input_shape=train_X_lst[0].shape) \n",
    "\n",
    "# # adding layers to process the maps\n",
    "# # \n",
    "# model.add_node(Convolution2D(64, 3, 3, activation='relu', border_mode='same', dim_ordering='th')\n",
    "#                , name='map_conv1', input='maps')\n",
    "# # add pooling layers\n",
    "# model.add_node(Flatten(), name='map_flatten', input='map_conv1')\n",
    "# model.add_node(Dense(64), name='map_dense1', input='map_flatten')\n",
    "\n",
    "# # adding layers to process the lst\n",
    "# model.add_node(Dense(16), name='lst_dense1', input='lst') \n",
    "# model.add_node(Dense(8), name='lst_dense2', input='lst_dense1')\n",
    "\n",
    "# # merging two sets of weights\n",
    "# model.add_node(Dense(72, activation='relu'), name='combine', inputs=['map_flatten', 'lst_dense1'], merge_mode='concat')\n",
    "# model.add_node(Dense(1, activation='sigmoid'), name='reduce', input='combine')\n",
    "# model.add_output(name='output', input='reduce')\n",
    "\n",
    "# sgd = SGD(lr=0.001)\n",
    "# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy']) \n",
    "\n",
    "# # use dropouts \n",
    "# # try different solver\n",
    "# # if acc = 0, network diverged, reduce step size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# model.fit({'maps': train_X_map, 'lst': train_X_lst, 'output': train_y}, nb_epoch=10, batch_size=10, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# prediction = model.predict({'maps': np.array(test_X_map), 'lst': np.array(test_X_lst)}, batch_size=1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
