{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This non-commercial license of GraphLab Create is assigned to tcchiao@gmail.com and will expire on April 16, 2017. For commercial licensing options, visit https://dato.com/buy/.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-04-15 18:31:58,185 [INFO] graphlab.cython.cy_server, 176: GraphLab Create v1.8.5 started. Logging: /tmp/graphlab_server_1460770316.log\n",
      "//anaconda/lib/python2.7/site-packages/theano/tensor/signal/downsample.py:6: UserWarning: downsample module has been moved to the theano.tensor.signal.pool module.\n",
      "  \"downsample module has been moved to the theano.tensor.signal.pool module.\")\n"
     ]
    }
   ],
   "source": [
    "# import utility libraries\n",
    "from netCDF4 import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.basemap import Basemap, cm\n",
    "import subprocess\n",
    "import os\n",
    "from scipy.misc import imread, imresize, imsave\n",
    "from scipy.optimize import fmin_l_bfgs_b\n",
    "import time\n",
    "import argparse\n",
    "import h5py\n",
    "from datetime import datetime\n",
    "%matplotlib inline\n",
    "\n",
    "# import machine learning tools\n",
    "import graphlab as gl\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Convolution2D, ZeroPadding2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "\n",
    "# import utilities and classes I wrote\n",
    "from clustering import Location_Clusterer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_folder = '/Users/Chiao/google-drive/projects/Galvanize/fall-foliage-finder/data/'\n",
    "\n",
    "# Getting my labels\n",
    "nc = Dataset(data_folder+'sign.label.nc', 'r')\n",
    "labels = nc.variables['Band1'][4:][np.newaxis]\n",
    "lons = nc.variables['lon'][:]\n",
    "lats = nc.variables['lat'][:]\n",
    "times = nc.variables['time'][:]\n",
    "\n",
    "nc = Dataset(data_folder+'all.ndvi.nc')\n",
    "ndvis = nc.variables['Band1'][:]\n",
    "\n",
    "ndvis = np.ma.concatenate((ndvis[2:-2][np.newaxis], ndvis[1:-3][np.newaxis], ndvis[0:-4][np.newaxis]), axis=0)\n",
    "mask = ndvis.mask[:]\n",
    "\n",
    "ndvis = np.lib.pad(ndvis, ((0, 0), (0, 0), (5, 5), (5, 5)), 'constant', constant_values=(-3000))\n",
    "\n",
    "nobs = len(times)*len(lats)*len(lons)\n",
    "\n",
    "t0 = datetime.now()\n",
    "flatten_data = np.zeros((nobs, 3, 10,10))\n",
    "n = 0\n",
    "for i, t in enumerate(times):\n",
    "    for j, lat in enumerate(lats):\n",
    "        for k, lon in enumerate(lons):\n",
    "            flatten_data[n, :] = np.squeeze(ndvis[:, i, j:j+10, k:k+10])\n",
    "            n += 1\n",
    "print 'that took:', datetime.now()-t0, 'seconds'\n",
    "            \n",
    "\n",
    "# nc = Dataset(data_folder+'elev.nc')\n",
    "# elev = nc.variables['elev'][:][np.newaxis][np.newaxis]\n",
    "# print elev.shape\n",
    "\n",
    "# nc = Dataset(data_folder+'veg.nc')\n",
    "# veg = nc.variables['Cv'][:][np.newaxis]\n",
    "# print veg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(437, 614, 927)\n",
      "(437, 614, 927)\n"
     ]
    }
   ],
   "source": [
    "class NN_Input(object):\n",
    "    def __init__(self, data_folder):\n",
    "        self.folder = data_folder\n",
    "        self.data = {}\n",
    "        \n",
    "    def variables(self):\n",
    "        return self.data.keys()\n",
    "\n",
    "    def load(f_paths, names):\n",
    "        for f, v in zip(f_paths, names):\n",
    "            nc = Dataset(data_folder+f_path, 'r')\n",
    "            k = nc.variables.keys()\n",
    "            self.data[v] = nc.variables[k[-1]][:]\n",
    "\n",
    "\n",
    "f_paths = ['all.ndvi.nc', 'sign.label.nc', 'elev.nc']\n",
    "names = ['ndvi', 'label', 'elev']\n",
    "\n",
    "\n",
    "# util function to open, resize and format pictures into appropriate tensors\n",
    "def preprocess_image(image_path):\n",
    "    img = imresize(imread(image_path), (img_width, img_height))\n",
    "    img = img.transpose((2, 0, 1)).astype('float64')\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    return img\n",
    "\n",
    "# util function to convert a tensor into a valid image\n",
    "def deprocess_image(x):\n",
    "    x = x.transpose((1, 2, 0))\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # subsetting the first 100 time points to work with\n",
    "# # 100 time points is around 2 years of data\n",
    "# n = 100\n",
    "\n",
    "# # flatten the 3D array into a 1D column\n",
    "# coords = np.meshgrid(times[:n], lats[10:-10], lons[10:-10], indexing='ij')\n",
    "# nobs = n*len(lats[10:-10])*len(lons[10:-10])\n",
    "# flatten_labels = np.zeros((nobs, len(coords)+2))\n",
    "\n",
    "# for i in xrange(len(coords)):\n",
    "#     flatten_labels[:,i] = coords[i].flatten()\n",
    "    \n",
    "# # get the label from 2 timestamps away \n",
    "# flatten_labels[:,-2] = labels[2:n+2, 10:-10, 10:-10].flatten()\n",
    "\n",
    "# # stack the mask together from current time point and the future time point\n",
    "# # get the composite\n",
    "# current_mask = labels.mask[:n, 10:-10, 10:-10].flatten()\n",
    "# future_mask = labels.mask[2:n+2, 10:-10, 10:-10].flatten()\n",
    "# flatten_labels[:,-1] = np.any(np.vstack((current_mask, future_mask)), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# base_image = data['ndvi'][:20]\n",
    "# result_prefix = 'result'\n",
    "\n",
    "# # dimensions of the generated picture.\n",
    "# img_width, img_height = data['label'].shape[-2:]\n",
    "\n",
    "\n",
    "\n",
    "# # build the VGG16 network\n",
    "# model = Sequential()\n",
    "\n",
    "# model.add(ZeroPadding2D((1, 1), batch_input_shape=(1, 3, img_width, img_height)))\n",
    "\n",
    "# model.add(Convolution2D(64, 3, 3, activation='relu', name='conv1_1'))\n",
    "# model.add(ZeroPadding2D((1, 1)))\n",
    "# model.add(Convolution2D(64, 3, 3, activation='relu', name='conv1_2'))\n",
    "# model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "# model.add(ZeroPadding2D((1, 1)))\n",
    "# model.add(Convolution2D(128, 3, 3, activation='relu', name='conv2_1'))\n",
    "# model.add(ZeroPadding2D((1, 1)))\n",
    "# model.add(Convolution2D(128, 3, 3, activation='relu', name='conv2_2'))\n",
    "# model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "\n",
    "# # get the symbolic outputs of each \"key\" layer (we gave them unique names).\n",
    "# layer_dict = dict([(layer.name, layer) for layer in model.layers])\n",
    "\n",
    "# # continuity loss util function\n",
    "# def continuity_loss(x):\n",
    "#     assert K.ndim(x) == 4\n",
    "#     a = K.square(x[:, :, :img_width-1, :img_height-1] - x[:, :, 1:, :img_height-1])\n",
    "#     b = K.square(x[:, :, :img_width-1, :img_height-1] - x[:, :, :img_width-1, 1:])\n",
    "#     return K.sum(K.pow(a + b, 1.25))\n",
    "\n",
    "# # define the loss\n",
    "# loss = K.variable(0.)\n",
    "# for layer_name in settings['features']:\n",
    "#     # add the L2 norm of the features of a layer to the loss\n",
    "#     assert layer_name in layer_dict.keys(), 'Layer ' + layer_name + ' not found in model.'\n",
    "#     coeff = settings['features'][layer_name]\n",
    "#     x = layer_dict[layer_name].output\n",
    "#     shape = layer_dict[layer_name].output_shape\n",
    "#     # we avoid border artifacts by only involving non-border pixels in the loss\n",
    "#     loss -= coeff * K.sum(K.square(x[:, :, 2: shape[2]-2, 2: shape[3]-2])) / np.prod(shape[1:])\n",
    "\n",
    "# # add continuity loss (gives image local coherence, can result in an artful blur)\n",
    "# loss += settings['continuity'] * continuity_loss(dream) / (3 * img_width * img_height)\n",
    "# # add image L2 norm to loss (prevents pixels from taking very high values, makes image darker)\n",
    "# loss += settings['dream_l2'] * K.sum(K.square(dream)) / (3 * img_width * img_height)\n",
    "\n",
    "# # feel free to further modify the loss as you see fit, to achieve new effects...\n",
    "\n",
    "# # compute the gradients of the dream wrt the loss\n",
    "# grads = K.gradients(loss, dream)\n",
    "\n",
    "# outputs = [loss]\n",
    "# if type(grads) in {list, tuple}:\n",
    "#     outputs += grads\n",
    "# else:\n",
    "#     outputs.append(grads)\n",
    "\n",
    "# f_outputs = K.function([dream], outputs)\n",
    "# def eval_loss_and_grads(x):\n",
    "#     x = x.reshape((1, 3, img_width, img_height))\n",
    "#     outs = f_outputs([x])\n",
    "#     loss_value = outs[0]\n",
    "#     if len(outs[1:]) == 1:\n",
    "#         grad_values = outs[1].flatten().astype('float64')\n",
    "#     else:\n",
    "#         grad_values = np.array(outs[1:]).flatten().astype('float64')\n",
    "#     return loss_value, grad_values\n",
    "\n",
    "# # this Evaluator class makes it possible\n",
    "# # to compute loss and gradients in one pass\n",
    "# # while retrieving them via two separate functions,\n",
    "# # \"loss\" and \"grads\". This is done because scipy.optimize\n",
    "# # requires separate functions for loss and gradients,\n",
    "# # but computing them separately would be inefficient.\n",
    "# class Evaluator(object):\n",
    "#     def __init__(self):\n",
    "#         self.loss_value = None\n",
    "#         self.grads_values = None\n",
    "\n",
    "#     def loss(self, x):\n",
    "#         assert self.loss_value is None\n",
    "#         loss_value, grad_values = eval_loss_and_grads(x)\n",
    "#         self.loss_value = loss_value\n",
    "#         self.grad_values = grad_values\n",
    "#         return self.loss_value\n",
    "\n",
    "#     def grads(self, x):\n",
    "#         assert self.loss_value is not None\n",
    "#         grad_values = np.copy(self.grad_values)\n",
    "#         self.loss_value = None\n",
    "#         self.grad_values = None\n",
    "#         return grad_values\n",
    "\n",
    "# evaluator = Evaluator()\n",
    "\n",
    "# # run scipy-based optimization (L-BFGS) over the pixels of the generated image\n",
    "# # so as to minimize the loss\n",
    "# x = preprocess_image(base_image_path)\n",
    "# for i in range(5):\n",
    "#     print('Start of iteration', i)\n",
    "#     start_time = time.time()\n",
    "\n",
    "#     # add a random jitter to the initial image. This will be reverted at decoding time\n",
    "#     random_jitter = (settings['jitter'] * 2) * (np.random.random((3, img_width, img_height)) - 0.5)\n",
    "#     x += random_jitter\n",
    "\n",
    "#     # run L-BFGS for 7 steps\n",
    "#     x, min_val, info = fmin_l_bfgs_b(evaluator.loss, x.flatten(),\n",
    "#                                      fprime=evaluator.grads, maxfun=7)\n",
    "#     print('Current loss value:', min_val)\n",
    "#     # decode the dream and save it\n",
    "#     x = x.reshape((3, img_width, img_height))\n",
    "#     x -= random_jitter\n",
    "#     img = deprocess_image(x)\n",
    "#     fname = result_prefix + '_at_iteration_%d.png' % i\n",
    "#     imsave(fname, img)\n",
    "#     end_time = time.time()\n",
    "#     print('Image saved as', fname)\n",
    "#     print('Iteration %d completed in %ds' % (i, end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
