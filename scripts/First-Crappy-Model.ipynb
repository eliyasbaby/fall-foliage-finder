{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "# import utility libraries\n",
    "from netCDF4 import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.basemap import Basemap, cm\n",
    "%matplotlib inline\n",
    "\n",
    "# import machine learning tools\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential, Graph\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, ZeroPadding2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "\n",
    "# import utilities and classes I wrote\n",
    "from clustering import Location_Clusterer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting nn_input.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile nn_input.py\n",
    "# import utility libraries\n",
    "from netCDF4 import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "\n",
    "# import machine learning tools\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Convolution2D, ZeroPadding2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "\n",
    "# import utilities and classes I wrote\n",
    "from clustering import Location_Clusterer\n",
    "\n",
    "class NN_Input(object):\n",
    "    \"\"\"\n",
    "    Stores the input data ready for feeding into a keras neural network. \n",
    "\n",
    "    To-Do:\n",
    "    - add function to take the clustering data in some ways\n",
    "    - change the output of \"select\" to fit the graph model of keras\n",
    "    - add function to return the actual lat, lon, and time based on indices\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, predict=2, history=2, box=5):\n",
    "        \"\"\"\n",
    "        Initialize a class for storing neural network input data. \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        predict: int, number of time points ahead that the model will predict. \n",
    "                 For example, if predict=2, the model will predict 2 time points away from the given time. \n",
    "        history: int, number of time points for which data would be included as input.\n",
    "                 For example, if data_length=3, the model will receive 3 time points worth of data (current time\n",
    "                 point, the previous time point, and the timep point before that).\n",
    "        \"\"\"\n",
    "        self.lons = None\n",
    "        self.lats = None\n",
    "        self.times = None\n",
    "        \n",
    "        self.labels = None\n",
    "        self.features = {}\n",
    "        self.feature_types = {}\n",
    "        self.variables = []\n",
    "        \n",
    "        self.predict = predict\n",
    "        self.history = history\n",
    "        self.box = box\n",
    "        \n",
    "    def load_labels(self, f_path, var):\n",
    "        \"\"\"\n",
    "        Load labels from netCDF file. \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        f_path: string\n",
    "        var: string\n",
    "        \"\"\"\n",
    "        nc = Dataset(f_path, 'r')\n",
    "        self.lons = nc.variables['lon'][:]\n",
    "        self.lats = nc.variables['lat'][:]\n",
    "        \n",
    "        self.times = nc.variables['time'][self.history:-self.predict]\n",
    "        n = self.predict + self.history\n",
    "        self.labels = nc.variables[var][n:,:,:]\n",
    "        \n",
    "    def load_features(self, f_path, var, name, feature_type):\n",
    "        \"\"\"\n",
    "        Load feature values from netCDF files. Stores feature type information. \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        f_path: string, path to input netCDF file.\n",
    "        var: string, variable name as appeared in the netCDF file. \n",
    "        name: string, name of the variable to be stored. \n",
    "        feature_type: string, must be one of the following: 'history_time_series', 'forecast_time_series', \n",
    "        'multi_layers', 'single_layer'\n",
    "        \"\"\"\n",
    "        nc = Dataset(f_path, 'r')\n",
    "        temp_data = nc.variables[var][:]\n",
    "        \n",
    "        # Storing information on whether the input features \n",
    "        self.feature_types[name] = feature_type\n",
    "        self.variables.append(name)\n",
    "        \n",
    "        if self.feature_types[name] == 'history_time_series':\n",
    "            self.features[name] = temp_data[:-self.predict, :, :]\n",
    "        elif self.feature_types[name] == 'forecast_time_series':\n",
    "            self.features[name] = temp_data[self.history:, :, :]\n",
    "        else:\n",
    "            self.features[name] = temp_data\n",
    "        \n",
    "    \n",
    "    def get_features(self, i, j, k):\n",
    "        \"\"\"\n",
    "        Given indices for latitude, longitude, and time point, returns the associated data from self.data. \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        lat: int, index for the latitude desired. Must be within the range available in self.data. \n",
    "        lon: int, index for the longitude desired. Must be within the range available in self.data. \n",
    "        time: int, index for the time point desired. Must be within the range available in self.data. \n",
    "        \"\"\"\n",
    "        maps = None\n",
    "        lst = None\n",
    "        for ix, feat in enumerate(self.variables):\n",
    "            if self.feature_types[feat] == 'history_time_series':\n",
    "                temp_data = self.features[feat][i:i+self.history+1, j-self.box:j+self.box+1, k-self.box:k+self.box+1]\n",
    "            elif self.feature_types[feat] == 'forecast_time_series':\n",
    "                temp_data = self.features[feat][i:i+self.predict+1, j-self.box:j+self.box+1, k-self.box:k+self.box+1]\n",
    "            elif self.feature_types[feat] == 'multi_layers':\n",
    "                temp_data = self.features[feat][:, j, k].flatten()\n",
    "            else: \n",
    "                temp_data = self.features[feat][j, k]\n",
    "            \n",
    "            if len(temp_data.shape) == 3:                \n",
    "                if np.any(temp_data.mask):\n",
    "                    return None\n",
    "                elif maps is None:\n",
    "                    maps = temp_data\n",
    "                else:\n",
    "                    maps = np.ma.concatenate((maps, temp_data), axis=0)\n",
    "            else:\n",
    "                if lst is None:\n",
    "                    lst = temp_data\n",
    "                else:\n",
    "                    lst = np.append(lst, temp_data)\n",
    "        return [maps, lst]\n",
    "        \n",
    "    def select(self, n, cutoff=None):\n",
    "        if cutoff is None:\n",
    "            cutoff = len(self.times)/2\n",
    "            \n",
    "        indices, labels, output_maps, output_lst = [], [], [], []\n",
    "        \n",
    "        while len(labels) < n:\n",
    "            i = np.random.randint(cutoff)\n",
    "            j = np.random.randint(self.box, len(self.lats)-self.box)\n",
    "            k = np.random.randint(self.box, len(self.lons)-self.box)\n",
    "            features = self.get_features(i, j, k)\n",
    "            if features is not None:\n",
    "                indices.append([i, j, k])\n",
    "                labels.append(self.labels[i, j, k])\n",
    "                output_maps.append(features[0])\n",
    "                output_lst.append(features[1])\n",
    "        return np.array(indices), np.array(labels), np.array(output_maps), np.array(output_lst)\n",
    "\n",
    "    def _check_mask(self, i, j, k):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nn_input import NN_Input\n",
    "\n",
    "# Preparing a graph model neural network input\n",
    "\n",
    "folder = '/home/ubuntu/dataset/'\n",
    "\n",
    "nn = NN_Input(predict=2, history=2, box=20)\n",
    "nn.load_labels(folder+'sign.label.nc', 'Band1')\n",
    "\n",
    "f_paths = ['all.ndvi.nc','all.max.of.Wind.nc', 'all.min.of.Tmin.nc', 'all.mean.of.Tmin.nc', 'all.sum.of.Prec.nc',\n",
    "           'all.max.of.Tmax.nc', 'all.mean.of.Tmax.nc','elev.nc', 'veg.nc']\n",
    "variables = ['Band1', 'Wind', 'Tmin', 'Tmin', 'Prec', 'Tmax', 'Tmax', 'elev', 'Cv']\n",
    "names = ['ndvi', 'max_wind', 'min_tmin', 'mean_tmin', 'total_prec', 'max_tmax', 'mean_tmax', 'elev', 'veg']\n",
    "feature_types = ['history_time_series', 'forecast_time_series', 'forecast_time_series', 'forecast_time_series',\n",
    "                 'forecast_time_series', 'forecast_time_series', 'forecast_time_series',\n",
    "                'single_layer', 'multi_layers']\n",
    "\n",
    "for f_path, v, n, feature_type in zip(f_paths, variables, names, feature_types):\n",
    "    nn.load_features(folder+f_path, v, n, feature_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python2.7/site-packages/numpy/ma/core.py:4085: UserWarning: Warning: converting a masked element to nan.\n",
      "  warnings.warn(\"Warning: converting a masked element to nan.\")\n"
     ]
    }
   ],
   "source": [
    "train_id, train_y, train_X_map, train_X_lst = nn.select(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21, 41, 41)\n",
      "(12,)\n",
      "<type 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print train_X_map[0].shape\n",
    "print train_X_lst[0].shape\n",
    "print type(train_X_map)\n",
    "#print np.array(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# graph = Graph() \n",
    "# graph.add_input(name='input1', input_shape=(32,)) \n",
    "# graph.add_input(name='input2', input_shape=(32,)) \n",
    "# graph.add_node(Dense(16), name='dense1', input='input1') \n",
    "# graph.add_node(Dense(4), name='dense2', input='input2') \n",
    "# graph.add_node(Dense(4), name='dense3', input='dense1') \n",
    "# graph.add_output(name='output', inputs=['dense2', 'dense3'], merge_mode='sum') \n",
    "# graph.compile(optimizer='rmsprop', loss={'output':'mse'}) \n",
    "# history = graph.fit({'input1':X_train, 'input2':X2_train, 'output':y_train}, nb_epoch=10) \n",
    "# predictions = graph.predict({'input1':X_test, 'input2':X2_test}) # {'output':...}\n",
    "\n",
    "train_y[train_y == -1] = 0\n",
    "\n",
    "\n",
    "# graph model with two inputs and one output \n",
    "model = Graph() \n",
    "\n",
    "map_dimensions=train_X_map[0][0].shape\n",
    "\n",
    "# two types of inputs: maps in 3D matrix and a list\n",
    "model.add_input(name='maps', input_shape=train_X_map[0].shape) \n",
    "model.add_input(name='lst', input_shape=train_X_lst[0].shape) \n",
    "\n",
    "# adding layers to process the maps\n",
    "model.add_node(Convolution2D(32, 3, 3, activation='relu', border_mode='same', input_shape=(21,41,41), dim_ordering='th')\n",
    "               , name='map_conv1', input='maps')\n",
    "model.add_node(Convolution2D(16, 3, 3, activation='relu', border_mode='same', dim_ordering='th')\n",
    "               , name='map_conv2', input='map_conv1')\n",
    "model.add_node(Flatten(), name='map_flatten', input='map_conv2')\n",
    "model.add_node(Dense(32), name='map_dense1', input='map_flatten')\n",
    "\n",
    "# adding layers to process the lst\n",
    "model.add_node(Dense(16), name='lst_dense1', input='lst') \n",
    "model.add_node(Dense(8), name='lst_dense2', input='lst_dense1')\n",
    "\n",
    "# merging two sets of weights\n",
    "model.add_node(Dense(40, activation='relu'), name='combine', inputs=['map_flatten', 'lst_dense1'], merge_mode='concat')\n",
    "model.add_node(Dense(1, activation='softmax'), name='reduce', input='combine')\n",
    "model.add_output(name='output', input='reduce')\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='mse', metrics=['accuracy']) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 21, 41, 41)\n",
      "(1000, 12)\n",
      "(1000,)\n",
      "Epoch 1/3\n",
      "1000/1000 [==============================] - 14s - loss: nan - acc: 0.2960    \n",
      "Epoch 2/3\n",
      "1000/1000 [==============================] - 14s - loss: nan - acc: 0.0000e+00    \n",
      "Epoch 3/3\n",
      "1000/1000 [==============================] - 14s - loss: nan - acc: 0.0000e+00    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8b5dd73910>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print train_X_map.shape\n",
    "print train_X_lst.shape\n",
    "print train_y.shape\n",
    "\n",
    "model.fit({'maps': train_X_map, 'lst': train_X_lst, 'output': train_y}, nb_epoch=3, batch_size=10, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
