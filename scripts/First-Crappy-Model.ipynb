{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import utility libraries\n",
    "from netCDF4 import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.basemap import Basemap, cm\n",
    "%matplotlib inline\n",
    "\n",
    "# import machine learning tools\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Convolution2D, ZeroPadding2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "\n",
    "# import utilities and classes I wrote\n",
    "from clustering import Location_Clusterer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NN_Input(object):\n",
    "    \"\"\"\n",
    "    Stores the input data ready for feeding into a keras neural network. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data_files: list; \n",
    "    \n",
    "    To-do:\n",
    "    - look into the graph model in keras.  \n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, predict=2, history=3):\n",
    "        \"\"\"\n",
    "        Initialize a class for storing neural network input data. \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        predict: int, number of time points ahead that the model will predict. \n",
    "                 For example, if predict=1, the model will predict the next time point of the input data. \n",
    "        history: int, number of time points for which data would be included as input.\n",
    "                 For example, if data_length=3, the model will receive 3 time points worth of data (current time\n",
    "                 point, the previous time point, and the timep point before that).\n",
    "        \"\"\"\n",
    "        self.lons = None\n",
    "        self.lats = None\n",
    "        self.times = None\n",
    "        \n",
    "        self.labels = None\n",
    "        self.features = {}\n",
    "        self.feature_types = {}\n",
    "        \n",
    "        self.predict = predict\n",
    "        self.history = history\n",
    "        \n",
    "    def load_labels(self, f_path, var):\n",
    "        \"\"\"\n",
    "        Load labels from netCDF file. \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        f_path: string\n",
    "        var: string\n",
    "        \"\"\"\n",
    "        nc = Dataset(f_path, 'r')\n",
    "        self.lons = nc.variables['lon'][:]\n",
    "        self.lats = nc.variables['lat'][:]\n",
    "        \n",
    "        self.times = nc.variables['time'][self.history-1:-self.predict]\n",
    "        n = self.predict + self.history - 1\n",
    "        self.labels = nc.variables[var][n:,:,:]\n",
    "        \n",
    "    def load_features(self, f_path, var, name, feature_type):\n",
    "        \"\"\"\n",
    "        Load feature values from netCDF files. Stores feature type information. \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        f_path: string, path to input netCDF file.\n",
    "        var: string, variable name as appeared in the netCDF file. \n",
    "        name: string, name of the variable to be stored. \n",
    "        feature_type: string, must be one of the following: 'time_series', 'multi_layers', 'single_layer'\n",
    "        \"\"\"\n",
    "        nc = Dataset(f_path, 'r')\n",
    "        temp_data = nc.variables[var][:]\n",
    "        \n",
    "        # Storing information on whether the input features \n",
    "        self.feature_types[name] = feature_type\n",
    "        \n",
    "        if self.feature_types[name] == 'time_series':\n",
    "            self.features[name] = temp_data[:-self.predict, :, :]\n",
    "        else:\n",
    "            self.features[name] = temp_data\n",
    "        \n",
    "    def _read_data(self, f_path, var, time_series=True):\n",
    "        \"\"\"\n",
    "        Reads in data from netCDF file. \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        f_path: string, path to a netCDF input file. The file must have dimensions lat, lon.\n",
    "        var: string, name of the variable to be loaded, must have dimension lat x lon if time_series=False.\n",
    "             Must have dimension lat x lon x time if time_series=True. \n",
    "        time_series: boolean, indicates whether there's a \"time\" dimension to the data. \n",
    "        \"\"\"\n",
    "        nc = Dataset(f_path, 'r')\n",
    "        temp_data = nc.variables[var][:]\n",
    "        \n",
    "        if time_series:\n",
    "            times = nc.variables['time'][:]\n",
    "            return [(times, lats, lons), temp_data]\n",
    "        \n",
    "        return [(lats, lons), temp_data]\n",
    "    \n",
    "    def _get_features(self, lat, lon, time):\n",
    "        \"\"\"\n",
    "        Given indices for latitude, longitude, and time point, returns the associated data from self.data. \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        lat: int, index for the latitude desired. Must be within the range available in self.data. \n",
    "        lon: int, index for the longitude desired. Must be within the range available in self.data. \n",
    "        time: int, index for the time point desired. Must be within the range available in self.data. \n",
    "        \"\"\"\n",
    "        \n",
    "    def select(self, n, cutoff=None):\n",
    "        if cutoff is None:\n",
    "            cutoff = len(self.times)/2\n",
    "        \n",
    "        # from shape self.times[:cutoff] x self.lat x self.lon\n",
    "        # initiate list of output\n",
    "        # while len(list) < n:\n",
    "            # randomly select 1 point\n",
    "            # get features\n",
    "            # check if in mask or if there are null values in the features (>10000 or <-2000?)\n",
    "            # if yes disgard and move on \n",
    "            # if no append to list\n",
    "        # save the list as array (self.ready_input)\n",
    "        # save the corresponding label into an array. \n",
    "    \n",
    "    def variables(self):\n",
    "        return self.data.keys()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_folder = '/Users/Chiao/google-drive/projects/Galvanize/fall-foliage-finder/data/'\n",
    "\n",
    "# Getting my labels\n",
    "nc = Dataset(data_folder+'sign.label.nc', 'r')\n",
    "labels = nc.variables['Band1'][4:][np.newaxis]\n",
    "lons = nc.variables['lon'][:]\n",
    "lats = nc.variables['lat'][:]\n",
    "times = nc.variables['time'][:]\n",
    "\n",
    "nc = Dataset(data_folder+'all.ndvi.nc')\n",
    "ndvis = nc.variables['Band1'][:]\n",
    "\n",
    "ndvis = np.ma.concatenate((ndvis[2:-2][np.newaxis], ndvis[1:-3][np.newaxis], ndvis[0:-4][np.newaxis]), axis=0)\n",
    "mask = ndvis.mask[:]\n",
    "\n",
    "ndvis = np.lib.pad(ndvis, ((0, 0), (0, 0), (5, 5), (5, 5)), 'constant', constant_values=(-3000))\n",
    "\n",
    "nobs = len(times)*len(lats)*len(lons)\n",
    "\n",
    "t0 = datetime.now()\n",
    "flatten_data = np.zeros((nobs, 3, 10,10))\n",
    "n = 0\n",
    "for i, t in enumerate(times):\n",
    "    for j, lat in enumerate(lats):\n",
    "        for k, lon in enumerate(lons):\n",
    "            flatten_data[n, :] = np.squeeze(ndvis[:, i, j:j+10, k:k+10])\n",
    "            n += 1\n",
    "print 'that took:', datetime.now()-t0, 'seconds'\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(437, 614, 927)\n",
      "(437, 614, 927)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "f_paths = ['all.ndvi.nc', 'sign.label.nc', 'elev.nc']\n",
    "names = ['ndvi', 'label', 'elev']\n",
    "\n",
    "\n",
    "# util function to open, resize and format pictures into appropriate tensors\n",
    "def preprocess_image(image_path):\n",
    "    img = imresize(imread(image_path), (img_width, img_height))\n",
    "    img = img.transpose((2, 0, 1)).astype('float64')\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    return img\n",
    "\n",
    "# util function to convert a tensor into a valid image\n",
    "def deprocess_image(x):\n",
    "    x = x.transpose((1, 2, 0))\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # subsetting the first 100 time points to work with\n",
    "# # 100 time points is around 2 years of data\n",
    "# n = 100\n",
    "\n",
    "# # flatten the 3D array into a 1D column\n",
    "# coords = np.meshgrid(times[:n], lats[10:-10], lons[10:-10], indexing='ij')\n",
    "# nobs = n*len(lats[10:-10])*len(lons[10:-10])\n",
    "# flatten_labels = np.zeros((nobs, len(coords)+2))\n",
    "\n",
    "# for i in xrange(len(coords)):\n",
    "#     flatten_labels[:,i] = coords[i].flatten()\n",
    "    \n",
    "# # get the label from 2 timestamps away \n",
    "# flatten_labels[:,-2] = labels[2:n+2, 10:-10, 10:-10].flatten()\n",
    "\n",
    "# # stack the mask together from current time point and the future time point\n",
    "# # get the composite\n",
    "# current_mask = labels.mask[:n, 10:-10, 10:-10].flatten()\n",
    "# future_mask = labels.mask[2:n+2, 10:-10, 10:-10].flatten()\n",
    "# flatten_labels[:,-1] = np.any(np.vstack((current_mask, future_mask)), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# base_image = data['ndvi'][:20]\n",
    "# result_prefix = 'result'\n",
    "\n",
    "# # dimensions of the generated picture.\n",
    "# img_width, img_height = data['label'].shape[-2:]\n",
    "\n",
    "\n",
    "\n",
    "# # build the VGG16 network\n",
    "# model = Sequential()\n",
    "\n",
    "# model.add(ZeroPadding2D((1, 1), batch_input_shape=(1, 3, img_width, img_height)))\n",
    "\n",
    "# model.add(Convolution2D(64, 3, 3, activation='relu', name='conv1_1'))\n",
    "# model.add(ZeroPadding2D((1, 1)))\n",
    "# model.add(Convolution2D(64, 3, 3, activation='relu', name='conv1_2'))\n",
    "# model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "# model.add(ZeroPadding2D((1, 1)))\n",
    "# model.add(Convolution2D(128, 3, 3, activation='relu', name='conv2_1'))\n",
    "# model.add(ZeroPadding2D((1, 1)))\n",
    "# model.add(Convolution2D(128, 3, 3, activation='relu', name='conv2_2'))\n",
    "# model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "\n",
    "# # get the symbolic outputs of each \"key\" layer (we gave them unique names).\n",
    "# layer_dict = dict([(layer.name, layer) for layer in model.layers])\n",
    "\n",
    "# # continuity loss util function\n",
    "# def continuity_loss(x):\n",
    "#     assert K.ndim(x) == 4\n",
    "#     a = K.square(x[:, :, :img_width-1, :img_height-1] - x[:, :, 1:, :img_height-1])\n",
    "#     b = K.square(x[:, :, :img_width-1, :img_height-1] - x[:, :, :img_width-1, 1:])\n",
    "#     return K.sum(K.pow(a + b, 1.25))\n",
    "\n",
    "# # define the loss\n",
    "# loss = K.variable(0.)\n",
    "# for layer_name in settings['features']:\n",
    "#     # add the L2 norm of the features of a layer to the loss\n",
    "#     assert layer_name in layer_dict.keys(), 'Layer ' + layer_name + ' not found in model.'\n",
    "#     coeff = settings['features'][layer_name]\n",
    "#     x = layer_dict[layer_name].output\n",
    "#     shape = layer_dict[layer_name].output_shape\n",
    "#     # we avoid border artifacts by only involving non-border pixels in the loss\n",
    "#     loss -= coeff * K.sum(K.square(x[:, :, 2: shape[2]-2, 2: shape[3]-2])) / np.prod(shape[1:])\n",
    "\n",
    "# # add continuity loss (gives image local coherence, can result in an artful blur)\n",
    "# loss += settings['continuity'] * continuity_loss(dream) / (3 * img_width * img_height)\n",
    "# # add image L2 norm to loss (prevents pixels from taking very high values, makes image darker)\n",
    "# loss += settings['dream_l2'] * K.sum(K.square(dream)) / (3 * img_width * img_height)\n",
    "\n",
    "# # feel free to further modify the loss as you see fit, to achieve new effects...\n",
    "\n",
    "# # compute the gradients of the dream wrt the loss\n",
    "# grads = K.gradients(loss, dream)\n",
    "\n",
    "# outputs = [loss]\n",
    "# if type(grads) in {list, tuple}:\n",
    "#     outputs += grads\n",
    "# else:\n",
    "#     outputs.append(grads)\n",
    "\n",
    "# f_outputs = K.function([dream], outputs)\n",
    "# def eval_loss_and_grads(x):\n",
    "#     x = x.reshape((1, 3, img_width, img_height))\n",
    "#     outs = f_outputs([x])\n",
    "#     loss_value = outs[0]\n",
    "#     if len(outs[1:]) == 1:\n",
    "#         grad_values = outs[1].flatten().astype('float64')\n",
    "#     else:\n",
    "#         grad_values = np.array(outs[1:]).flatten().astype('float64')\n",
    "#     return loss_value, grad_values\n",
    "\n",
    "# # this Evaluator class makes it possible\n",
    "# # to compute loss and gradients in one pass\n",
    "# # while retrieving them via two separate functions,\n",
    "# # \"loss\" and \"grads\". This is done because scipy.optimize\n",
    "# # requires separate functions for loss and gradients,\n",
    "# # but computing them separately would be inefficient.\n",
    "# class Evaluator(object):\n",
    "#     def __init__(self):\n",
    "#         self.loss_value = None\n",
    "#         self.grads_values = None\n",
    "\n",
    "#     def loss(self, x):\n",
    "#         assert self.loss_value is None\n",
    "#         loss_value, grad_values = eval_loss_and_grads(x)\n",
    "#         self.loss_value = loss_value\n",
    "#         self.grad_values = grad_values\n",
    "#         return self.loss_value\n",
    "\n",
    "#     def grads(self, x):\n",
    "#         assert self.loss_value is not None\n",
    "#         grad_values = np.copy(self.grad_values)\n",
    "#         self.loss_value = None\n",
    "#         self.grad_values = None\n",
    "#         return grad_values\n",
    "\n",
    "# evaluator = Evaluator()\n",
    "\n",
    "# # run scipy-based optimization (L-BFGS) over the pixels of the generated image\n",
    "# # so as to minimize the loss\n",
    "# x = preprocess_image(base_image_path)\n",
    "# for i in range(5):\n",
    "#     print('Start of iteration', i)\n",
    "#     start_time = time.time()\n",
    "\n",
    "#     # add a random jitter to the initial image. This will be reverted at decoding time\n",
    "#     random_jitter = (settings['jitter'] * 2) * (np.random.random((3, img_width, img_height)) - 0.5)\n",
    "#     x += random_jitter\n",
    "\n",
    "#     # run L-BFGS for 7 steps\n",
    "#     x, min_val, info = fmin_l_bfgs_b(evaluator.loss, x.flatten(),\n",
    "#                                      fprime=evaluator.grads, maxfun=7)\n",
    "#     print('Current loss value:', min_val)\n",
    "#     # decode the dream and save it\n",
    "#     x = x.reshape((3, img_width, img_height))\n",
    "#     x -= random_jitter\n",
    "#     img = deprocess_image(x)\n",
    "#     fname = result_prefix + '_at_iteration_%d.png' % i\n",
    "#     imsave(fname, img)\n",
    "#     end_time = time.time()\n",
    "#     print('Image saved as', fname)\n",
    "#     print('Iteration %d completed in %ds' % (i, end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
