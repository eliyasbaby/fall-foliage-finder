{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "# import utility libraries\n",
    "from netCDF4 import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.basemap import Basemap, cm\n",
    "%matplotlib inline\n",
    "\n",
    "# import machine learning tools\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential, Graph\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, ZeroPadding2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "\n",
    "# import utilities and classes I wrote\n",
    "from clustering import Location_Clusterer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting nn_input.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile nn_input.py\n",
    "# import utility libraries\n",
    "from netCDF4 import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "\n",
    "# import machine learning tools\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Convolution2D, ZeroPadding2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "\n",
    "# import utilities and classes I wrote\n",
    "from clustering import Location_Clusterer\n",
    "\n",
    "class NN_Input(object):\n",
    "    \"\"\"\n",
    "    Stores the input data ready for feeding into a keras neural network. \n",
    "\n",
    "    To-Do:\n",
    "    - add function to take the clustering data in some ways\n",
    "    - change the output of \"select\" to fit the graph model of keras\n",
    "    - add function to return the actual lat, lon, and time based on indices\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, predict=2, history=2, box=5):\n",
    "        \"\"\"\n",
    "        Initialize a class for storing neural network input data. \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        predict: int, number of time points ahead that the model will predict. \n",
    "                 For example, if predict=2, the model will predict 2 time points away from the given time. \n",
    "        history: int, number of time points for which data would be included as input.\n",
    "                 For example, if data_length=3, the model will receive 3 time points worth of data (current time\n",
    "                 point, the previous time point, and the timep point before that).\n",
    "        \"\"\"\n",
    "        self.lons = None\n",
    "        self.lats = None\n",
    "        self.times = None\n",
    "        \n",
    "        self.labels = None\n",
    "        self.features = {}\n",
    "        self.feature_types = {}\n",
    "        self.variables = []\n",
    "        \n",
    "        self.predict = predict\n",
    "        self.history = history\n",
    "        self.box = box\n",
    "        \n",
    "    def load_labels(self, f_path, var):\n",
    "        \"\"\"\n",
    "        Load labels from netCDF file. \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        f_path: string\n",
    "        var: string\n",
    "        \"\"\"\n",
    "        nc = Dataset(f_path, 'r')\n",
    "        self.lons = nc.variables['lon'][:]\n",
    "        self.lats = nc.variables['lat'][:]\n",
    "        \n",
    "        self.times = nc.variables['time'][self.history:-self.predict]\n",
    "        n = self.predict + self.history\n",
    "        self.labels = nc.variables[var][n:,:,:]\n",
    "        \n",
    "    def load_features(self, f_path, var, name, feature_type):\n",
    "        \"\"\"\n",
    "        Load feature values from netCDF files. Stores feature type information. \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        f_path: string, path to input netCDF file.\n",
    "        var: string, variable name as appeared in the netCDF file. \n",
    "        name: string, name of the variable to be stored. \n",
    "        feature_type: string, must be one of the following: 'history_time_series', 'forecast_time_series', \n",
    "        'multi_layers', 'single_layer'\n",
    "        \"\"\"\n",
    "        nc = Dataset(f_path, 'r')\n",
    "        temp_data = nc.variables[var][:]\n",
    "        \n",
    "        # Storing information on whether the input features \n",
    "        self.feature_types[name] = feature_type\n",
    "        self.variables.append(name)\n",
    "        \n",
    "        if self.feature_types[name] == 'history_time_series':\n",
    "            self.features[name] = temp_data[:-self.predict, :, :]\n",
    "        elif self.feature_types[name] == 'forecast_time_series':\n",
    "            self.features[name] = temp_data[self.history:, :, :]\n",
    "        else:\n",
    "            self.features[name] = temp_data\n",
    "        \n",
    "    \n",
    "    def get_features(self, i, j, k):\n",
    "        \"\"\"\n",
    "        Given indices for latitude, longitude, and time point, returns the associated data from self.data. \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        lat: int, index for the latitude desired. Must be within the range available in self.data. \n",
    "        lon: int, index for the longitude desired. Must be within the range available in self.data. \n",
    "        time: int, index for the time point desired. Must be within the range available in self.data. \n",
    "        \"\"\"\n",
    "        maps = None\n",
    "        lst = None\n",
    "        for ix, feat in enumerate(self.variables):\n",
    "            if self.feature_types[feat] == 'history_time_series':\n",
    "                temp_data = self.features[feat][i:i+self.history+1, j-self.box:j+self.box+1, k-self.box:k+self.box+1]\n",
    "            elif self.feature_types[feat] == 'forecast_time_series':\n",
    "                temp_data = self.features[feat][i:i+self.predict+1, j-self.box:j+self.box+1, k-self.box:k+self.box+1]\n",
    "            elif self.feature_types[feat] == 'multi_layers':\n",
    "                temp_data = self.features[feat][:, j, k].flatten()\n",
    "            else: \n",
    "                temp_data = self.features[feat][j, k]\n",
    "            \n",
    "            if len(temp_data.shape) == 3:                \n",
    "                if np.any(temp_data.mask):\n",
    "                    return None\n",
    "                elif maps is None:\n",
    "                    maps = temp_data\n",
    "                else:\n",
    "                    maps = np.ma.concatenate((maps, temp_data), axis=0)\n",
    "            else:\n",
    "                if lst is None:\n",
    "                    lst = temp_data\n",
    "                else:\n",
    "                    lst = np.append(lst, temp_data)\n",
    "        return [maps, lst]\n",
    "        \n",
    "    def select(self, n, cutoff=None):\n",
    "        if cutoff is None:\n",
    "            cutoff = len(self.times)/2\n",
    "            \n",
    "        indices, labels, output_maps, output_lst = [], [], [], []\n",
    "        \n",
    "        while len(labels) < n:\n",
    "            i = np.random.randint(cutoff)\n",
    "            j = np.random.randint(self.box, len(self.lats)-self.box)\n",
    "            k = np.random.randint(self.box, len(self.lons)-self.box)\n",
    "            features = self.get_features(i, j, k)\n",
    "            if features is not None:\n",
    "                indices.append([i, j, k])\n",
    "                labels.append(self.labels[i, j, k])\n",
    "                output_maps.append(features[0])\n",
    "                output_lst.append(features[1])\n",
    "        return np.array(indices), np.array(labels), np.array(output_maps), np.array(output_lst)\n",
    "\n",
    "    def _check_mask(self, i, j, k):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nn_input import NN_Input\n",
    "\n",
    "# Preparing a graph model neural network input\n",
    "\n",
    "folder = '/home/ubuntu/dataset/'\n",
    "\n",
    "nn = NN_Input(predict=2, history=2, box=20)\n",
    "nn.load_labels(folder+'sign.label.nc', 'Band1')\n",
    "\n",
    "f_paths = ['all.ndvi.nc','all.max.of.Wind.nc', 'all.min.of.Tmin.nc', 'all.mean.of.Tmin.nc', 'all.sum.of.Prec.nc',\n",
    "           'all.max.of.Tmax.nc', 'all.mean.of.Tmax.nc','elev.nc', 'veg.nc']\n",
    "variables = ['Band1', 'Wind', 'Tmin', 'Tmin', 'Prec', 'Tmax', 'Tmax', 'elev', 'Cv']\n",
    "names = ['ndvi', 'max_wind', 'min_tmin', 'mean_tmin', 'total_prec', 'max_tmax', 'mean_tmax', 'elev', 'veg']\n",
    "feature_types = ['history_time_series', 'forecast_time_series', 'forecast_time_series', 'forecast_time_series',\n",
    "                 'forecast_time_series', 'forecast_time_series', 'forecast_time_series',\n",
    "                'single_layer', 'multi_layers']\n",
    "\n",
    "for f_path, v, n, feature_type in zip(f_paths, variables, names, feature_types):\n",
    "    nn.load_features(folder+f_path, v, n, feature_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_id, train_y, train_X_map, train_X_lst = nn.select(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21, 41, 41)\n",
      "(12,)\n",
      "<type 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print train_X_map[0].shape\n",
    "print train_X_lst[0].shape\n",
    "print type(train_X_map)\n",
    "#print np.array(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# graph = Graph() \n",
    "# graph.add_input(name='input1', input_shape=(32,)) \n",
    "# graph.add_input(name='input2', input_shape=(32,)) \n",
    "# graph.add_node(Dense(16), name='dense1', input='input1') \n",
    "# graph.add_node(Dense(4), name='dense2', input='input2') \n",
    "# graph.add_node(Dense(4), name='dense3', input='dense1') \n",
    "# graph.add_output(name='output', inputs=['dense2', 'dense3'], merge_mode='sum') \n",
    "# graph.compile(optimizer='rmsprop', loss={'output':'mse'}) \n",
    "# history = graph.fit({'input1':X_train, 'input2':X2_train, 'output':y_train}, nb_epoch=10) \n",
    "# predictions = graph.predict({'input1':X_test, 'input2':X2_test}) # {'output':...}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# graph model with two inputs and one output \n",
    "model = Graph() \n",
    "\n",
    "map_dimensions=train_X_map[0][0].shape\n",
    "\n",
    "# two types of inputs: maps in 3D matrix and a list\n",
    "model.add_input(name='maps', input_shape=train_X_map[0].shape) \n",
    "model.add_input(name='lst', input_shape=train_X_lst[0].shape) \n",
    "\n",
    "# adding layers to process the maps\n",
    "model.add_node(Convolution2D(32, 3, 3, activation='relu', border_mode='same'), name='map_conv1', input='maps')\n",
    "model.add_node(Convolution2D(16, 3, 3, activation='relu', border_mode='same'), name='map_conv2', input='map_conv1')\n",
    "model.add_node(MaxPooling2D(pool_size=(2, 2)), name='map_pool1', input='map_conv2')\n",
    "model.add_node(Flatten(), name='map_flatten', input='map_pool1')\n",
    "\n",
    "# adding layers to process the lst\n",
    "model.add_node(Dense(10), name='lst_dense1', input='lst') \n",
    "model.add_node(Dense(10), name='lst_dense2', input='lst_dense1')\n",
    "\n",
    "# merging two sets of weights\n",
    "model.add_node(Dense(24, activation='relu'), name='combine', inputs=['map_flatten', 'lst_dense1'], merge_mode='concat')\n",
    "\n",
    "#model.add_node(Dense(1, activation='softmax'), name='output', input='combine')\n",
    "model.add_output(name='output', input='combine')\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='mse') \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 21, 41, 41)\n",
      "(10, 12)\n",
      "(10,)\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input dimension mis-match. (input[1].shape[1] = 24, input[4].shape[1] = 1)\nApply node that caused the error: Elemwise{Composite{((i0 * (Abs(i1) + i2 + i3)) - i4)}}[(0, 2)](TensorConstant{(1, 1) of 0.5}, Elemwise{add,no_inplace}.0, Dot22.0, InplaceDimShuffle{x,0}.0, output_target)\nToposort index: 79\nInputs types: [TensorType(float32, (True, True)), TensorType(float32, matrix), TensorType(float32, matrix), TensorType(float32, row), TensorType(float32, matrix)]\nInputs shapes: [(1, 1), (10, 24), (10, 24), (1, 24), (10, 1)]\nInputs strides: [(4, 4), (96, 4), (96, 4), (96, 4), (4, 4)]\nInputs values: [array([[ 0.5]], dtype=float32), 'not shown', 'not shown', 'not shown', 'not shown']\nOutputs clients: [[Elemwise{sqr,no_inplace}(Elemwise{Composite{((i0 * (Abs(i1) + i2 + i3)) - i4)}}[(0, 2)].0), Elemwise{Composite{(((i0 * i1 * i2) / i3) + ((i0 * i1 * i2 * sgn(i4)) / i3))}}[(0, 2)](InplaceDimShuffle{x,x}.0, InplaceDimShuffle{0,x}.0, Elemwise{Composite{((i0 * (Abs(i1) + i2 + i3)) - i4)}}[(0, 2)].0, Elemwise{Mul}[(0, 0)].0, Elemwise{add,no_inplace}.0)]]\n\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-22de77705d83>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'maps'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtrain_X_map\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'lst'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtrain_X_lst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'output'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/ubuntu/anaconda/lib/python2.7/site-packages/keras/legacy/models.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, data, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, **kwargs)\u001b[0m\n\u001b[0;32m    436\u001b[0m                                       \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m                                       \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 438\u001b[1;33m                                       sample_weight=sample_weight)\n\u001b[0m\u001b[0;32m    439\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    440\u001b[0m     def evaluate(self, data, batch_size=128,\n",
      "\u001b[1;32m/home/ubuntu/anaconda/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight)\u001b[0m\n\u001b[0;32m   1028\u001b[0m                               \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1029\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1030\u001b[1;33m                               callback_metrics=callback_metrics)\n\u001b[0m\u001b[0;32m   1031\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1032\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ubuntu/anaconda/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, nb_epoch, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics)\u001b[0m\n\u001b[0;32m    766\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    767\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 768\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    769\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    770\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ubuntu/anaconda/lib/python2.7/site-packages/keras/backend/theano_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    491\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 493\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    494\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ubuntu/anaconda/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    869\u001b[0m                     \u001b[0mnode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mposition_of_error\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    870\u001b[0m                     \u001b[0mthunk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mthunk\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 871\u001b[1;33m                     storage_map=getattr(self.fn, 'storage_map', None))\n\u001b[0m\u001b[0;32m    872\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    873\u001b[0m                 \u001b[1;31m# old-style linkers raise their own exceptions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ubuntu/anaconda/lib/python2.7/site-packages/theano/gof/link.pyc\u001b[0m in \u001b[0;36mraise_with_op\u001b[1;34m(node, thunk, exc_info, storage_map)\u001b[0m\n\u001b[0;32m    312\u001b[0m         \u001b[1;31m# extra long error message in that case.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 314\u001b[1;33m     \u001b[0mreraise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_trace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    315\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ubuntu/anaconda/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    860\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'position_of_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input dimension mis-match. (input[1].shape[1] = 24, input[4].shape[1] = 1)\nApply node that caused the error: Elemwise{Composite{((i0 * (Abs(i1) + i2 + i3)) - i4)}}[(0, 2)](TensorConstant{(1, 1) of 0.5}, Elemwise{add,no_inplace}.0, Dot22.0, InplaceDimShuffle{x,0}.0, output_target)\nToposort index: 79\nInputs types: [TensorType(float32, (True, True)), TensorType(float32, matrix), TensorType(float32, matrix), TensorType(float32, row), TensorType(float32, matrix)]\nInputs shapes: [(1, 1), (10, 24), (10, 24), (1, 24), (10, 1)]\nInputs strides: [(4, 4), (96, 4), (96, 4), (96, 4), (4, 4)]\nInputs values: [array([[ 0.5]], dtype=float32), 'not shown', 'not shown', 'not shown', 'not shown']\nOutputs clients: [[Elemwise{sqr,no_inplace}(Elemwise{Composite{((i0 * (Abs(i1) + i2 + i3)) - i4)}}[(0, 2)].0), Elemwise{Composite{(((i0 * i1 * i2) / i3) + ((i0 * i1 * i2 * sgn(i4)) / i3))}}[(0, 2)](InplaceDimShuffle{x,x}.0, InplaceDimShuffle{0,x}.0, Elemwise{Composite{((i0 * (Abs(i1) + i2 + i3)) - i4)}}[(0, 2)].0, Elemwise{Mul}[(0, 0)].0, Elemwise{add,no_inplace}.0)]]\n\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node."
     ]
    }
   ],
   "source": [
    "print train_X_map.shape\n",
    "print train_X_lst.shape\n",
    "print train_y.shape\n",
    "\n",
    "model.fit({'maps': train_X_map, 'lst': train_X_lst, 'output': train_y}, nb_epoch=3, batch_size=10, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # subsetting the first 100 time points to work with\n",
    "# # 100 time points is around 2 years of data\n",
    "# n = 100\n",
    "\n",
    "# # flatten the 3D array into a 1D column\n",
    "# coords = np.meshgrid(times[:n], lats[10:-10], lons[10:-10], indexing='ij')\n",
    "# nobs = n*len(lats[10:-10])*len(lons[10:-10])\n",
    "# flatten_labels = np.zeros((nobs, len(coords)+2))\n",
    "\n",
    "# for i in xrange(len(coords)):\n",
    "#     flatten_labels[:,i] = coords[i].flatten()\n",
    "    \n",
    "# # get the label from 2 timestamps away \n",
    "# flatten_labels[:,-2] = labels[2:n+2, 10:-10, 10:-10].flatten()\n",
    "\n",
    "# # stack the mask together from current time point and the future time point\n",
    "# # get the composite\n",
    "# current_mask = labels.mask[:n, 10:-10, 10:-10].flatten()\n",
    "# future_mask = labels.mask[2:n+2, 10:-10, 10:-10].flatten()\n",
    "# flatten_labels[:,-1] = np.any(np.vstack((current_mask, future_mask)), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# base_image = data['ndvi'][:20]\n",
    "# result_prefix = 'result'\n",
    "\n",
    "# # dimensions of the generated picture.\n",
    "# img_width, img_height = data['label'].shape[-2:]\n",
    "\n",
    "\n",
    "\n",
    "# # build the VGG16 network\n",
    "# model = Sequential()\n",
    "\n",
    "# model.add(ZeroPadding2D((1, 1), batch_input_shape=(1, 3, img_width, img_height)))\n",
    "\n",
    "# model.add(Convolution2D(64, 3, 3, activation='relu', name='conv1_1'))\n",
    "# model.add(ZeroPadding2D((1, 1)))\n",
    "# model.add(Convolution2D(64, 3, 3, activation='relu', name='conv1_2'))\n",
    "# model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "# model.add(ZeroPadding2D((1, 1)))\n",
    "# model.add(Convolution2D(128, 3, 3, activation='relu', name='conv2_1'))\n",
    "# model.add(ZeroPadding2D((1, 1)))\n",
    "# model.add(Convolution2D(128, 3, 3, activation='relu', name='conv2_2'))\n",
    "# model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "\n",
    "# # get the symbolic outputs of each \"key\" layer (we gave them unique names).\n",
    "# layer_dict = dict([(layer.name, layer) for layer in model.layers])\n",
    "\n",
    "# # continuity loss util function\n",
    "# def continuity_loss(x):\n",
    "#     assert K.ndim(x) == 4\n",
    "#     a = K.square(x[:, :, :img_width-1, :img_height-1] - x[:, :, 1:, :img_height-1])\n",
    "#     b = K.square(x[:, :, :img_width-1, :img_height-1] - x[:, :, :img_width-1, 1:])\n",
    "#     return K.sum(K.pow(a + b, 1.25))\n",
    "\n",
    "# # define the loss\n",
    "# loss = K.variable(0.)\n",
    "# for layer_name in settings['features']:\n",
    "#     # add the L2 norm of the features of a layer to the loss\n",
    "#     assert layer_name in layer_dict.keys(), 'Layer ' + layer_name + ' not found in model.'\n",
    "#     coeff = settings['features'][layer_name]\n",
    "#     x = layer_dict[layer_name].output\n",
    "#     shape = layer_dict[layer_name].output_shape\n",
    "#     # we avoid border artifacts by only involving non-border pixels in the loss\n",
    "#     loss -= coeff * K.sum(K.square(x[:, :, 2: shape[2]-2, 2: shape[3]-2])) / np.prod(shape[1:])\n",
    "\n",
    "# # add continuity loss (gives image local coherence, can result in an artful blur)\n",
    "# loss += settings['continuity'] * continuity_loss(dream) / (3 * img_width * img_height)\n",
    "# # add image L2 norm to loss (prevents pixels from taking very high values, makes image darker)\n",
    "# loss += settings['dream_l2'] * K.sum(K.square(dream)) / (3 * img_width * img_height)\n",
    "\n",
    "# # feel free to further modify the loss as you see fit, to achieve new effects...\n",
    "\n",
    "# # compute the gradients of the dream wrt the loss\n",
    "# grads = K.gradients(loss, dream)\n",
    "\n",
    "# outputs = [loss]\n",
    "# if type(grads) in {list, tuple}:\n",
    "#     outputs += grads\n",
    "# else:\n",
    "#     outputs.append(grads)\n",
    "\n",
    "# f_outputs = K.function([dream], outputs)\n",
    "# def eval_loss_and_grads(x):\n",
    "#     x = x.reshape((1, 3, img_width, img_height))\n",
    "#     outs = f_outputs([x])\n",
    "#     loss_value = outs[0]\n",
    "#     if len(outs[1:]) == 1:\n",
    "#         grad_values = outs[1].flatten().astype('float64')\n",
    "#     else:\n",
    "#         grad_values = np.array(outs[1:]).flatten().astype('float64')\n",
    "#     return loss_value, grad_values\n",
    "\n",
    "# # this Evaluator class makes it possible\n",
    "# # to compute loss and gradients in one pass\n",
    "# # while retrieving them via two separate functions,\n",
    "# # \"loss\" and \"grads\". This is done because scipy.optimize\n",
    "# # requires separate functions for loss and gradients,\n",
    "# # but computing them separately would be inefficient.\n",
    "# class Evaluator(object):\n",
    "#     def __init__(self):\n",
    "#         self.loss_value = None\n",
    "#         self.grads_values = None\n",
    "\n",
    "#     def loss(self, x):\n",
    "#         assert self.loss_value is None\n",
    "#         loss_value, grad_values = eval_loss_and_grads(x)\n",
    "#         self.loss_value = loss_value\n",
    "#         self.grad_values = grad_values\n",
    "#         return self.loss_value\n",
    "\n",
    "#     def grads(self, x):\n",
    "#         assert self.loss_value is not None\n",
    "#         grad_values = np.copy(self.grad_values)\n",
    "#         self.loss_value = None\n",
    "#         self.grad_values = None\n",
    "#         return grad_values\n",
    "\n",
    "# evaluator = Evaluator()\n",
    "\n",
    "# # run scipy-based optimization (L-BFGS) over the pixels of the generated image\n",
    "# # so as to minimize the loss\n",
    "# x = preprocess_image(base_image_path)\n",
    "# for i in range(5):\n",
    "#     print('Start of iteration', i)\n",
    "#     start_time = time.time()\n",
    "\n",
    "#     # add a random jitter to the initial image. This will be reverted at decoding time\n",
    "#     random_jitter = (settings['jitter'] * 2) * (np.random.random((3, img_width, img_height)) - 0.5)\n",
    "#     x += random_jitter\n",
    "\n",
    "#     # run L-BFGS for 7 steps\n",
    "#     x, min_val, info = fmin_l_bfgs_b(evaluator.loss, x.flatten(),\n",
    "#                                      fprime=evaluator.grads, maxfun=7)\n",
    "#     print('Current loss value:', min_val)\n",
    "#     # decode the dream and save it\n",
    "#     x = x.reshape((3, img_width, img_height))\n",
    "#     x -= random_jitter\n",
    "#     img = deprocess_image(x)\n",
    "#     fname = result_prefix + '_at_iteration_%d.png' % i\n",
    "#     imsave(fname, img)\n",
    "#     end_time = time.time()\n",
    "#     print('Image saved as', fname)\n",
    "#     print('Iteration %d completed in %ds' % (i, end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
