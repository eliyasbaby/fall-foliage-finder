{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import utility libraries\n",
    "from netCDF4 import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.basemap import Basemap, cm\n",
    "%matplotlib inline\n",
    "\n",
    "# import machine learning tools\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential, Graph\n",
    "from keras.layers import Dense\n",
    "from keras.layers.convolutional import Convolution2D, ZeroPadding2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "\n",
    "# import utilities and classes I wrote\n",
    "from clustering import Location_Clusterer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting nn_input.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile nn_input.py\n",
    "# import utility libraries\n",
    "from netCDF4 import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "\n",
    "# import machine learning tools\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Convolution2D, ZeroPadding2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "\n",
    "# import utilities and classes I wrote\n",
    "from clustering import Location_Clusterer\n",
    "\n",
    "class NN_Input(object):\n",
    "    \"\"\"\n",
    "    Stores the input data ready for feeding into a keras neural network. \n",
    "\n",
    "    To-Do:\n",
    "    - add function to take the clustering data in some ways\n",
    "    - change the output of \"select\" to fit the graph model of keras\n",
    "    - add function to return the actual lat, lon, and time based on indices\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, predict=2, history=2, box=5):\n",
    "        \"\"\"\n",
    "        Initialize a class for storing neural network input data. \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        predict: int, number of time points ahead that the model will predict. \n",
    "                 For example, if predict=2, the model will predict 2 time points away from the given time. \n",
    "        history: int, number of time points for which data would be included as input.\n",
    "                 For example, if data_length=3, the model will receive 3 time points worth of data (current time\n",
    "                 point, the previous time point, and the timep point before that).\n",
    "        \"\"\"\n",
    "        self.lons = None\n",
    "        self.lats = None\n",
    "        self.times = None\n",
    "        \n",
    "        self.labels = None\n",
    "        self.features = {}\n",
    "        self.feature_types = {}\n",
    "        self.variables = []\n",
    "        \n",
    "        self.predict = predict\n",
    "        self.history = history\n",
    "        self.box = box\n",
    "        \n",
    "    def load_labels(self, f_path, var):\n",
    "        \"\"\"\n",
    "        Load labels from netCDF file. \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        f_path: string\n",
    "        var: string\n",
    "        \"\"\"\n",
    "        nc = Dataset(f_path, 'r')\n",
    "        self.lons = nc.variables['lon'][:]\n",
    "        self.lats = nc.variables['lat'][:]\n",
    "        \n",
    "        self.times = nc.variables['time'][self.history:-self.predict]\n",
    "        n = self.predict + self.history\n",
    "        self.labels = nc.variables[var][n:,:,:]\n",
    "        \n",
    "    def load_features(self, f_path, var, name, feature_type):\n",
    "        \"\"\"\n",
    "        Load feature values from netCDF files. Stores feature type information. \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        f_path: string, path to input netCDF file.\n",
    "        var: string, variable name as appeared in the netCDF file. \n",
    "        name: string, name of the variable to be stored. \n",
    "        feature_type: string, must be one of the following: 'history_time_series', 'forecast_time_series', \n",
    "        'multi_layers', 'single_layer'\n",
    "        \"\"\"\n",
    "        nc = Dataset(f_path, 'r')\n",
    "        temp_data = nc.variables[var][:]\n",
    "        \n",
    "        # Storing information on whether the input features \n",
    "        self.feature_types[name] = feature_type\n",
    "        self.variables.append(name)\n",
    "        \n",
    "        if self.feature_types[name] == 'history_time_series':\n",
    "            self.features[name] = temp_data[:-self.predict, :, :]\n",
    "        elif self.feature_types[name] == 'forecast_time_series':\n",
    "            self.features[name] = temp_data[self.history:, :, :]\n",
    "        else:\n",
    "            self.features[name] = temp_data\n",
    "        \n",
    "    \n",
    "    def get_features(self, i, j, k):\n",
    "        \"\"\"\n",
    "        Given indices for latitude, longitude, and time point, returns the associated data from self.data. \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        lat: int, index for the latitude desired. Must be within the range available in self.data. \n",
    "        lon: int, index for the longitude desired. Must be within the range available in self.data. \n",
    "        time: int, index for the time point desired. Must be within the range available in self.data. \n",
    "        \"\"\"\n",
    "        maps = None\n",
    "        lst = None\n",
    "        for ix, feat in enumerate(self.variables):\n",
    "            if self.feature_types[feat] == 'history_time_series':\n",
    "                temp_data = self.features[feat][i:i+self.history+1, j-self.box:j+self.box+1, k-self.box:k+self.box+1]\n",
    "            elif self.feature_types[feat] == 'forecast_time_series':\n",
    "                temp_data = self.features[feat][i:i+self.predict+1, j-self.box:j+self.box+1, k-self.box:k+self.box+1]\n",
    "            elif self.feature_types[feat] == 'multi_layers':\n",
    "                temp_data = self.features[feat][:, j, k].flatten()\n",
    "            else: \n",
    "                temp_data = self.features[feat][j, k]\n",
    "            \n",
    "            if len(temp_data.shape) == 3:                \n",
    "                if np.any(temp_data.mask):\n",
    "                    return None\n",
    "                elif maps is None:\n",
    "                    maps = temp_data\n",
    "                else:\n",
    "                    maps = np.ma.concatenate((maps, temp_data), axis=0)\n",
    "            else:\n",
    "                if lst is None:\n",
    "                    lst = temp_data\n",
    "                else:\n",
    "                    lst = np.append(lst, temp_data)\n",
    "        return [maps, lst]\n",
    "        \n",
    "    def select(self, n, cutoff=None):\n",
    "        if cutoff is None:\n",
    "            cutoff = len(self.times)/2\n",
    "            \n",
    "        indices, labels, output_maps, output_lst = [], [], [], []\n",
    "        \n",
    "        while len(labels) < n:\n",
    "            i = np.random.randint(cutoff)\n",
    "            j = np.random.randint(self.box, len(self.lats)-self.box)\n",
    "            k = np.random.randint(self.box, len(self.lons)-self.box)\n",
    "            features = self.get_features(i, j, k)\n",
    "            if features is not None:\n",
    "                indices.append([i, j, k])\n",
    "                labels.append(self.labels[i, j, k])\n",
    "                output_maps.append(features[0])\n",
    "                output_lst.append(features[1])\n",
    "        return indices, labels, output_maps, output_lst\n",
    "\n",
    "    def _check_mask(self, i, j, k):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nn_input import NN_Input\n",
    "\n",
    "# Preparing a graph model neural network input\n",
    "\n",
    "folder = '/home/ubuntu/dataset/'\n",
    "\n",
    "nn = NN_Input(predict=2, history=2, box=5)\n",
    "nn.load_labels(folder+'sign.label.nc', 'Band1')\n",
    "\n",
    "f_paths = ['all.ndvi.nc','all.max.of.Wind.nc', 'all.min.of.Tmin.nc', 'all.mean.of.Tmin.nc', 'all.sum.of.Prec.nc',\n",
    "           'all.max.of.Tmax.nc', 'all.mean.of.Tmax.nc','elev.nc', 'veg.nc']\n",
    "variables = ['Band1', 'Wind', 'Tmin', 'Tmin', 'Prec', 'Tmax', 'Tmax', 'elev', 'Cv']\n",
    "names = ['ndvi', 'max_wind', 'min_tmin', 'mean_tmin', 'total_prec', 'max_tmax', 'mean_tmax', 'elev', 'veg']\n",
    "feature_types = ['history_time_series', 'forecast_time_series', 'forecast_time_series', 'forecast_time_series',\n",
    "                 'forecast_time_series', 'forecast_time_series', 'forecast_time_series',\n",
    "                'single_layer', 'multi_layers']\n",
    "\n",
    "for f_path, v, n, feature_type in zip(f_paths, variables, names, feature_types):\n",
    "    nn.load_features(folder+f_path, v, n, feature_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_id, train_y, train_X_map, train_X_lst = nn.select(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-64d18f1ed310>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'input2'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_X_lst\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_node\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'dense1'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'input1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_node\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'dense2'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'input2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_node\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'dense3'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'dense1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ubuntu/anaconda/lib/python2.7/site-packages/keras/legacy/models.pyc\u001b[0m in \u001b[0;36madd_node\u001b[1;34m(self, layer, name, input, inputs, merge_mode, concat_axis, dot_axes, create_output)\u001b[0m\n\u001b[0;32m    169\u001b[0m                 \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_inbound_node\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_nodes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0minput\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_inputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 171\u001b[1;33m                 \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_inbound_node\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_inputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    172\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m             \u001b[0mto_merge\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ubuntu/anaconda/lib/python2.7/site-packages/keras/engine/topology.pyc\u001b[0m in \u001b[0;36madd_inbound_node\u001b[1;34m(self, inbound_layers, node_indices, tensor_indices)\u001b[0m\n\u001b[0;32m    535\u001b[0m             \u001b[1;31m# call build()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    536\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 537\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    538\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    539\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ubuntu/anaconda/lib/python2.7/site-packages/keras/layers/core.pyc\u001b[0m in \u001b[0;36mbuild\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m    582\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    583\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 584\u001b[1;33m         \u001b[1;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    585\u001b[0m         \u001b[0minput_dim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    586\u001b[0m         self.input_spec = [InputSpec(dtype=K.floatx(),\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# graph model with two inputs and one output \n",
    "graph = Graph() \n",
    "graph.add_input(name='input1', input_shape=train_X_map[0].shape, ndim=4) \n",
    "graph.add_input(name='input2', input_shape=train_X_lst[0].shape) \n",
    "\n",
    "graph.add_node(Dense(16), name='dense1', input='input1') \n",
    "graph.add_node(Dense(4), name='dense2', input='input2') \n",
    "graph.add_node(Dense(4), name='dense3', input='dense1') \n",
    "graph.add_output(name='output', inputs=['dense2', 'dense3'], merge_mode='sum') \n",
    "graph.compile(optimizer='rmsprop', loss={'output':'mse'}) \n",
    "\n",
    "history = graph.fit({'input1':X_train, 'input2':X2_train, 'output':y_train}, nb_epoch=10) \n",
    "predictions = graph.predict({'input1':X_test, 'input2':X2_test}) \n",
    "# {'output':...}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # subsetting the first 100 time points to work with\n",
    "# # 100 time points is around 2 years of data\n",
    "# n = 100\n",
    "\n",
    "# # flatten the 3D array into a 1D column\n",
    "# coords = np.meshgrid(times[:n], lats[10:-10], lons[10:-10], indexing='ij')\n",
    "# nobs = n*len(lats[10:-10])*len(lons[10:-10])\n",
    "# flatten_labels = np.zeros((nobs, len(coords)+2))\n",
    "\n",
    "# for i in xrange(len(coords)):\n",
    "#     flatten_labels[:,i] = coords[i].flatten()\n",
    "    \n",
    "# # get the label from 2 timestamps away \n",
    "# flatten_labels[:,-2] = labels[2:n+2, 10:-10, 10:-10].flatten()\n",
    "\n",
    "# # stack the mask together from current time point and the future time point\n",
    "# # get the composite\n",
    "# current_mask = labels.mask[:n, 10:-10, 10:-10].flatten()\n",
    "# future_mask = labels.mask[2:n+2, 10:-10, 10:-10].flatten()\n",
    "# flatten_labels[:,-1] = np.any(np.vstack((current_mask, future_mask)), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# base_image = data['ndvi'][:20]\n",
    "# result_prefix = 'result'\n",
    "\n",
    "# # dimensions of the generated picture.\n",
    "# img_width, img_height = data['label'].shape[-2:]\n",
    "\n",
    "\n",
    "\n",
    "# # build the VGG16 network\n",
    "# model = Sequential()\n",
    "\n",
    "# model.add(ZeroPadding2D((1, 1), batch_input_shape=(1, 3, img_width, img_height)))\n",
    "\n",
    "# model.add(Convolution2D(64, 3, 3, activation='relu', name='conv1_1'))\n",
    "# model.add(ZeroPadding2D((1, 1)))\n",
    "# model.add(Convolution2D(64, 3, 3, activation='relu', name='conv1_2'))\n",
    "# model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "# model.add(ZeroPadding2D((1, 1)))\n",
    "# model.add(Convolution2D(128, 3, 3, activation='relu', name='conv2_1'))\n",
    "# model.add(ZeroPadding2D((1, 1)))\n",
    "# model.add(Convolution2D(128, 3, 3, activation='relu', name='conv2_2'))\n",
    "# model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "\n",
    "# # get the symbolic outputs of each \"key\" layer (we gave them unique names).\n",
    "# layer_dict = dict([(layer.name, layer) for layer in model.layers])\n",
    "\n",
    "# # continuity loss util function\n",
    "# def continuity_loss(x):\n",
    "#     assert K.ndim(x) == 4\n",
    "#     a = K.square(x[:, :, :img_width-1, :img_height-1] - x[:, :, 1:, :img_height-1])\n",
    "#     b = K.square(x[:, :, :img_width-1, :img_height-1] - x[:, :, :img_width-1, 1:])\n",
    "#     return K.sum(K.pow(a + b, 1.25))\n",
    "\n",
    "# # define the loss\n",
    "# loss = K.variable(0.)\n",
    "# for layer_name in settings['features']:\n",
    "#     # add the L2 norm of the features of a layer to the loss\n",
    "#     assert layer_name in layer_dict.keys(), 'Layer ' + layer_name + ' not found in model.'\n",
    "#     coeff = settings['features'][layer_name]\n",
    "#     x = layer_dict[layer_name].output\n",
    "#     shape = layer_dict[layer_name].output_shape\n",
    "#     # we avoid border artifacts by only involving non-border pixels in the loss\n",
    "#     loss -= coeff * K.sum(K.square(x[:, :, 2: shape[2]-2, 2: shape[3]-2])) / np.prod(shape[1:])\n",
    "\n",
    "# # add continuity loss (gives image local coherence, can result in an artful blur)\n",
    "# loss += settings['continuity'] * continuity_loss(dream) / (3 * img_width * img_height)\n",
    "# # add image L2 norm to loss (prevents pixels from taking very high values, makes image darker)\n",
    "# loss += settings['dream_l2'] * K.sum(K.square(dream)) / (3 * img_width * img_height)\n",
    "\n",
    "# # feel free to further modify the loss as you see fit, to achieve new effects...\n",
    "\n",
    "# # compute the gradients of the dream wrt the loss\n",
    "# grads = K.gradients(loss, dream)\n",
    "\n",
    "# outputs = [loss]\n",
    "# if type(grads) in {list, tuple}:\n",
    "#     outputs += grads\n",
    "# else:\n",
    "#     outputs.append(grads)\n",
    "\n",
    "# f_outputs = K.function([dream], outputs)\n",
    "# def eval_loss_and_grads(x):\n",
    "#     x = x.reshape((1, 3, img_width, img_height))\n",
    "#     outs = f_outputs([x])\n",
    "#     loss_value = outs[0]\n",
    "#     if len(outs[1:]) == 1:\n",
    "#         grad_values = outs[1].flatten().astype('float64')\n",
    "#     else:\n",
    "#         grad_values = np.array(outs[1:]).flatten().astype('float64')\n",
    "#     return loss_value, grad_values\n",
    "\n",
    "# # this Evaluator class makes it possible\n",
    "# # to compute loss and gradients in one pass\n",
    "# # while retrieving them via two separate functions,\n",
    "# # \"loss\" and \"grads\". This is done because scipy.optimize\n",
    "# # requires separate functions for loss and gradients,\n",
    "# # but computing them separately would be inefficient.\n",
    "# class Evaluator(object):\n",
    "#     def __init__(self):\n",
    "#         self.loss_value = None\n",
    "#         self.grads_values = None\n",
    "\n",
    "#     def loss(self, x):\n",
    "#         assert self.loss_value is None\n",
    "#         loss_value, grad_values = eval_loss_and_grads(x)\n",
    "#         self.loss_value = loss_value\n",
    "#         self.grad_values = grad_values\n",
    "#         return self.loss_value\n",
    "\n",
    "#     def grads(self, x):\n",
    "#         assert self.loss_value is not None\n",
    "#         grad_values = np.copy(self.grad_values)\n",
    "#         self.loss_value = None\n",
    "#         self.grad_values = None\n",
    "#         return grad_values\n",
    "\n",
    "# evaluator = Evaluator()\n",
    "\n",
    "# # run scipy-based optimization (L-BFGS) over the pixels of the generated image\n",
    "# # so as to minimize the loss\n",
    "# x = preprocess_image(base_image_path)\n",
    "# for i in range(5):\n",
    "#     print('Start of iteration', i)\n",
    "#     start_time = time.time()\n",
    "\n",
    "#     # add a random jitter to the initial image. This will be reverted at decoding time\n",
    "#     random_jitter = (settings['jitter'] * 2) * (np.random.random((3, img_width, img_height)) - 0.5)\n",
    "#     x += random_jitter\n",
    "\n",
    "#     # run L-BFGS for 7 steps\n",
    "#     x, min_val, info = fmin_l_bfgs_b(evaluator.loss, x.flatten(),\n",
    "#                                      fprime=evaluator.grads, maxfun=7)\n",
    "#     print('Current loss value:', min_val)\n",
    "#     # decode the dream and save it\n",
    "#     x = x.reshape((3, img_width, img_height))\n",
    "#     x -= random_jitter\n",
    "#     img = deprocess_image(x)\n",
    "#     fname = result_prefix + '_at_iteration_%d.png' % i\n",
    "#     imsave(fname, img)\n",
    "#     end_time = time.time()\n",
    "#     print('Image saved as', fname)\n",
    "#     print('Iteration %d completed in %ds' % (i, end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
