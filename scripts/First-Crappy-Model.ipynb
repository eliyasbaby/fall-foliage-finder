{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: GRID K520 (CNMeM is disabled, cuDNN not available)\n"
     ]
    }
   ],
   "source": [
    "# import utility libraries\n",
    "from netCDF4 import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.basemap import Basemap, cm\n",
    "import cv2\n",
    "%matplotlib inline\n",
    "\n",
    "# import machine learning tools\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential, Graph\n",
    "from keras.layers import Dense, Flatten, Activation, Dropout\n",
    "from keras.layers.convolutional import Convolution2D, ZeroPadding2D, MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "from keras import backend as K\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "from clustering import Location_Clusterer, plot_list_in_2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "folder = '/home/ubuntu/dataset/'\n",
    "files = ['veg.nc', 'ppt.monthly.mask.nc', 'tmean.monthly.mask.nc', 'elev.nc']\n",
    "var_names = ['Cv', 'Band1', 'Band1', 'elev']\n",
    "\n",
    "lc = Location_Clusterer(n_clusters=n)\n",
    "for f, var in zip(files, var_names):\n",
    "    lc.read_data(folder+f, var)\n",
    "\n",
    "lc.transform_data()\n",
    "clusters = lc.fit_predict(lc.data2d)\n",
    "\n",
    "#plot_list_in_2D(lc.coords2d[:,0], lc.coords2d[:,1], clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# for i in xrange(n):\n",
    "#     ind = (clusters==i)\n",
    "#     plot_list_in_2D(lc.coords2d[:,0][ind], lc.coords2d[:,1][ind], clusters[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting nn_input.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile nn_input.py\n",
    "# import utility libraries\n",
    "from netCDF4 import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "\n",
    "# import machine learning tools\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Convolution2D, ZeroPadding2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "\n",
    "# import utilities and classes I wrote\n",
    "from clustering import Location_Clusterer\n",
    "\n",
    "class NN_Input(object):\n",
    "    \"\"\"\n",
    "    Stores the input data ready for feeding into a Keras neural network. \n",
    "\n",
    "    To-Do:\n",
    "    - add function to take the clustering data in some ways\n",
    "    - add function to return the actual lat, lon, and time based on indices\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, predict=2, history=2, box=5, random_seed=None):\n",
    "        \"\"\"\n",
    "        Initialize a class for storing neural network input data. \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        predict: int, number of time points ahead that the model will predict. \n",
    "                 For example, if predict=2, the model will predict 2 time points away from the given time. \n",
    "        history: int, number of time points for which data would be included as input.\n",
    "                 For example, if data_length=3, the model will receive 3 time points worth of data (current time\n",
    "                 point, the previous time point, and the timep point before that).\n",
    "        \"\"\"\n",
    "        if random_seed is not None:\n",
    "            np.random.seed(random_seed)\n",
    "        self.lons = None\n",
    "        self.lats = None\n",
    "        self.times = None\n",
    "        \n",
    "        self.labels = None\n",
    "        self.features = {}\n",
    "        self.feature_types = {}\n",
    "        self.variables = []\n",
    "        \n",
    "        self.predict = predict\n",
    "        self.history = history\n",
    "        self.box = box\n",
    "        \n",
    "    def load_labels(self, f_path, var):\n",
    "        \"\"\"\n",
    "        Load labels from netCDF file. \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        f_path: string\n",
    "        var: string\n",
    "        \"\"\"\n",
    "        nc = Dataset(f_path, 'r')\n",
    "        self.lons = nc.variables['lon'][:]\n",
    "        self.lats = nc.variables['lat'][:]\n",
    "        \n",
    "        self.times = nc.variables['time'][self.history:-self.predict]\n",
    "        n = self.predict + self.history\n",
    "        self.labels = nc.variables[var][n:,:,:]\n",
    "        \n",
    "    def load_features(self, f_path, var, name, feature_type):\n",
    "        \"\"\"\n",
    "        Load feature values from netCDF files. Stores feature type information. \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        f_path: string, path to input netCDF file.\n",
    "        var: string, variable name as appeared in the netCDF file. \n",
    "        name: string, name of the variable to be stored. \n",
    "        feature_type: string, must be one of the following: 'history_time_series', 'forecast_time_series', \n",
    "        'multi_layers', 'single_layer'\n",
    "        \"\"\"\n",
    "        nc = Dataset(f_path, 'r')\n",
    "        temp_data = nc.variables[var][:]\n",
    "        \n",
    "        # Storing information on whether the input features \n",
    "        self.feature_types[name] = feature_type\n",
    "        self.variables.append(name)\n",
    "        \n",
    "        if self.feature_types[name] == 'history_time_series':\n",
    "            self.features[name] = temp_data[:-self.predict, :, :]\n",
    "        elif self.feature_types[name] == 'forecast_time_series':\n",
    "            self.features[name] = temp_data[self.history:, :, :]\n",
    "        else:\n",
    "            self.features[name] = temp_data\n",
    "        \n",
    "    def get_batch(self, j, k):\n",
    "        pass\n",
    "    \n",
    "    def get_features(self, i, j, k):\n",
    "        \"\"\"\n",
    "        Given indices for latitude, longitude, and time point, returns the associated data from self.data. \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        lat: int, index for the latitude desired. Must be within the range available in self.data. \n",
    "        lon: int, index for the longitude desired. Must be within the range available in self.data. \n",
    "        time: int, index for the time point desired. Must be within the range available in self.data. \n",
    "        \"\"\"\n",
    "        maps = None\n",
    "#         lst = None\n",
    "        for ix, feat in enumerate(self.variables):\n",
    "            if self.feature_types[feat] == 'history_time_series':\n",
    "                temp_data = self.features[feat][i:i+self.history+1, j-self.box:j+self.box+1, k-self.box:k+self.box+1]\n",
    "            elif self.feature_types[feat] == 'forecast_time_series':\n",
    "                temp_data = self.features[feat][i:i+self.predict+1, j-self.box:j+self.box+1, k-self.box:k+self.box+1]\n",
    "#             elif self.feature_types[feat] == 'multi_layers':\n",
    "#                 temp_data = self.features[feat][:, j, k].flatten()\n",
    "#             else: \n",
    "#                 temp_data = self.features[feat][j, k]\n",
    "            \n",
    "            if len(temp_data.shape) == 3:                \n",
    "                if np.sum(temp_data.mask) > len(temp_data.flatten())/2:\n",
    "                    return None\n",
    "                elif np.any(temp_data.mask):\n",
    "                    temp_data = temp_data.filled(-999)\n",
    "                    \n",
    "                if maps is None:\n",
    "                    maps = temp_data\n",
    "                else:\n",
    "                    maps = np.ma.concatenate((maps, temp_data), axis=0)\n",
    "#             else:\n",
    "#                 if lst is None:\n",
    "#                     lst = temp_data\n",
    "#                 else:\n",
    "#                     lst = np.append(lst, temp_data)\n",
    "#         return [maps, lst]\n",
    "        return maps\n",
    "        \n",
    "    def select(self, n=None, t=None, cutoff=None, subset=None):\n",
    "        \"\"\"\n",
    "        Selecting n data points randomly from the database before specified time cutoff. \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        n: int, number of data points wanted. \n",
    "        cutoff: int, time cutoff for the training dataset. Default is half of the data available. \n",
    "        \"\"\"\n",
    "        if cutoff is None:\n",
    "            cutoff = len(self.times)/2\n",
    "        \n",
    "        indices, labels, output_maps, output_lst = [], [], [], []\n",
    "        \n",
    "        mi = 0\n",
    "        for ix, feat in enumerate(self.variables):\n",
    "            if self.feature_types[feat] == 'history_time_series':\n",
    "                mi += (self.history+1)\n",
    "            elif self.feature_types[feat] == 'forecast_time_series':\n",
    "                mi += (self.predict+1)\n",
    "                \n",
    "        map_dimensions = (mi, (2*self.box)+1, (2*self.box)+1)\n",
    "\n",
    "        if n is None and t is None:\n",
    "            for i in xrange(cutoff):\n",
    "                for (k, j) in subset:\n",
    "                    l = self.labels[i, j, k]\n",
    "                    features = self.get_features(i, j, k)\n",
    "                    if features is not None and l != np.nan and features.shape==map_dimensions:\n",
    "                        indices.append([i, j, k])\n",
    "                        labels.append(l)\n",
    "                        output_maps.append(features)\n",
    "#                         output_lst.append(features[1])\n",
    "        \n",
    "        elif t is not None:\n",
    "            for (k, j) in subset:\n",
    "                l = self.labels[t, j, k]\n",
    "                features = self.get_features(t, j, k)\n",
    "                if features is not None and l != np.nan and features.shape==map_dimensions:\n",
    "                    indices.append([t, j, k])\n",
    "                    labels.append(l)\n",
    "                    output_maps.append(features)\n",
    "#                     output_lst.append(features[1])\n",
    "        \n",
    "        else:\n",
    "            while len(labels) < n:\n",
    "                if subset is not None:\n",
    "                    (k, j) = subset[np.random.choice(len(subset))]\n",
    "                else:\n",
    "                    j = np.random.randint(self.box, len(self.lats)-self.box)\n",
    "                    k = np.random.randint(self.box, len(self.lons)-self.box)\n",
    "                i = np.random.randint(cutoff)\n",
    "                l = self.labels[i, j, k]\n",
    "                features = self.get_features(i, j, k)\n",
    "                if features is not None and l != np.nan and features.shape==map_dimensions:\n",
    "                    indices.append([i, j, k])\n",
    "                    labels.append(l)\n",
    "                    output_maps.append(features)\n",
    "#                     output_lst.append(features[1])\n",
    "                    \n",
    "        return np.array(indices), np.array(labels), np.array(output_maps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reformat_y(y):\n",
    "    y[y == -1] = 0\n",
    "    y = np.hstack((y.reshape(-1,1), 1-y.reshape(-1,1)))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nn_input import NN_Input\n",
    "\n",
    "nn = NN_Input(predict=2, history=5, box=5, random_seed=42)\n",
    "nn.load_labels(folder+'sign.label.nc', 'Band1')\n",
    "\n",
    "# f_paths = ['all.ndvi.nc','all.max.of.Wind.nc', 'all.min.of.Tmin.nc', 'all.mean.of.Tmin.nc', 'all.sum.of.Prec.nc',\n",
    "#            'all.max.of.Tmax.nc', 'all.mean.of.Tmax.nc','elev.nc', 'veg.nc']\n",
    "# variables = ['Band1', 'Wind', 'Tmin', 'Tmin', 'Prec', 'Tmax', 'Tmax', 'elev', 'Cv']\n",
    "# names = ['ndvi', 'max_wind', 'min_tmin', 'mean_tmin', 'total_prec', 'max_tmax', 'mean_tmax', 'elev', 'veg']\n",
    "# feature_types = ['history_time_series', 'forecast_time_series', 'forecast_time_series', 'forecast_time_series',\n",
    "#                  'forecast_time_series', 'forecast_time_series', 'forecast_time_series',\n",
    "#                 'single_layer', 'multi_layers']\n",
    "\n",
    "f_paths = ['all.mean.of.Tmin.nc', 'all.mean.of.Tmin.nc']\n",
    "variables = ['Tmin', 'Tmin']\n",
    "names = ['mean_tmin_history', 'mean_tmin_forecast']\n",
    "feature_types = ['history_time_series', 'forecast_time_series']\n",
    "\n",
    "for f_path, v, n, feature_type in zip(f_paths, variables, names, feature_types):\n",
    "    nn.load_features(folder+f_path, v, n, feature_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modeling for cluster 3\n",
      "Getting training dataset\n"
     ]
    }
   ],
   "source": [
    "point = [308, 290]\n",
    "for i, loc in enumerate(lc.ind2d):\n",
    "    if loc[0] == point[0] and loc[1] == point[1]:\n",
    "        cluster = clusters[i]\n",
    "\n",
    "print 'Modeling for cluster', cluster\n",
    "subset = lc.ind2d[clusters==cluster]\n",
    "\n",
    "print 'Getting training dataset'\n",
    "id_train, y_train, X_map_train = nn.select(n=100000, subset=subset)\n",
    "y_train = reformat_y(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting test dataset\n",
      "17859\n",
      "15879\n"
     ]
    }
   ],
   "source": [
    "print 'Getting test dataset'\n",
    "\n",
    "t = (len(nn.times)/2) +1\n",
    "print len(subset)\n",
    "id_test, y_test, X_map_test = nn.select(t=t, subset=subset)\n",
    "y_test = reformat_y(y_test)\n",
    "print len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 11, 11)\n",
      "False\n",
      "(100000, 2)\n"
     ]
    }
   ],
   "source": [
    "map_dimensions=X_map_train[0].shape\n",
    "print map_dimensions\n",
    "print np.all(X_map_train[0] == X_map_train[1])\n",
    "\n",
    "mean_train = np.mean(X_map_train.flatten())\n",
    "std_train = np.std(X_map_train.flatten())\n",
    "X_map_train = (X_map_train-mean_train)/std_train\n",
    "X_map_test = (X_map_test-mean_train)/std_train\n",
    "print y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 100000 samples, validate on 15879 samples\n",
      "Epoch 1/5\n",
      "100000/100000 [==============================] - 35s - loss: 0.4311 - acc: 0.8036 - val_loss: 0.8665 - val_acc: 0.6738\n",
      "Epoch 2/5\n",
      "100000/100000 [==============================] - 35s - loss: 0.2392 - acc: 0.9123 - val_loss: 0.7745 - val_acc: 0.6601\n",
      "Epoch 3/5\n",
      "100000/100000 [==============================] - 35s - loss: 0.1893 - acc: 0.9304 - val_loss: 0.6717 - val_acc: 0.6944\n",
      "Epoch 4/5\n",
      "100000/100000 [==============================] - 35s - loss: 0.1695 - acc: 0.9386 - val_loss: 0.6418 - val_acc: 0.6943\n",
      "Epoch 5/5\n",
      "100000/100000 [==============================] - 35s - loss: 0.1570 - acc: 0.9421 - val_loss: 0.8521 - val_acc: 0.6420\n",
      "100000/100000 [==============================] - 15s    \n",
      "15879/15879 [==============================] - 2s     \n"
     ]
    }
   ],
   "source": [
    "def build_base_sequential_NN(n_conv_layers=2, nb_filters=16, nb_conv=3, map_dimensions=None, nb_pool=2):\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(nb_filters, nb_conv, nb_conv,\n",
    "                            border_mode='valid',\n",
    "                            input_shape=map_dimensions))\n",
    "    \n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Convolution2D(nb_filters, nb_conv, nb_conv))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(nb_pool, nb_pool)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(2))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    sgd = SGD()\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=sgd,\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = build_base_sequential_NN(nb_filters=64, map_dimensions=map_dimensions)\n",
    "model.fit(X_map_train, y_train, batch_size=50, nb_epoch=5, verbose=True, validation_data=(X_map_test, y_test))\n",
    "train_predict = model.predict(X_map_train, verbose=True)\n",
    "test_predict = model.predict(X_map_test, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      "Bench mark: 0.65198\n",
      "Accuracy: 0.94727\n",
      "ROC AUC: 0.986955685883\n",
      "\n",
      "Testing set:\n",
      "Bench mark: 0.762831412557\n",
      "Accuracy: 0.64197997355\n",
      "ROC AUC: 0.621227554531\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.5\n",
    "\n",
    "print 'Training set:'\n",
    "print 'Bench mark:', np.sum(y_train[:,1])/float(len(y_train))\n",
    "print 'Accuracy:', accuracy_score(y_train[:,0], (train_predict[:,0]>threshold))\n",
    "print 'ROC AUC:', roc_auc_score(y_train[:,0], train_predict[:,0])\n",
    "print \n",
    "print 'Testing set:'\n",
    "print 'Bench mark:', np.sum(y_test[:,1])/float(len(y_test))\n",
    "print 'Accuracy:', accuracy_score(y_test[:,0], (test_predict[:,0]>threshold))\n",
    "print 'ROC AUC:', roc_auc_score(y_test[:,0], test_predict[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_compare_map(lons, lats, y_true, y_predict, timestamp, cmap=None, folder='/home/ubuntu/dataset/output/'):\n",
    "    timestamp = str(int(timestamp))\n",
    "    yr = timestamp[:4]\n",
    "    mn = timestamp[4:6]\n",
    "    dt = timestamp[6:]\n",
    "    \n",
    "    if cmap is None:\n",
    "        cmap=mpl.cm.get_cmap('RdYlGn')\n",
    "        \n",
    "    norm = mpl.colors.Normalize(y_true.min, y_true.max)\n",
    "    area_thresh=25000\n",
    "    land_color='grey'\n",
    "    ocean_color='lightblue'\n",
    "\n",
    "    fig = plt.figure(figsize=(10,7))\n",
    "    plt.title('/'.join([yr, mn, dt]), loc='right', fontsize=14)\n",
    "    plt.axis('off')\n",
    "    ax = fig.add_subplot(2,1,1)\n",
    "    m = Basemap(projection='cyl', llcrnrlat=lats.min(), llcrnrlon=lons.min(),\n",
    "            urcrnrlat=lats.max(), urcrnrlon=lons.max(), resolution='f',\n",
    "            area_thresh=area_thresh)\n",
    "    m.scatter(lons, lats, c=y_true, edgecolor='none', cmap=cmap)\n",
    "    m.drawlsmask(land_color=land_color, ocean_color=ocean_color, lakes=True)\n",
    "    m.drawcountries()\n",
    "    m.drawcoastlines()\n",
    "    \n",
    "    ax = fig.add_subplot(2,1,2)\n",
    "    m = Basemap(projection='cyl', llcrnrlat=lats.min(), llcrnrlon=lons.min(),\n",
    "            urcrnrlat=lats.max(), urcrnrlon=lons.max(), resolution='f',\n",
    "            area_thresh=area_thresh)\n",
    "    m.scatter(lons, lats, c=y_predict,edgecolor='none',cmap=cmap)\n",
    "    m.drawlsmask(land_color=land_color, ocean_color=ocean_color, lakes=True)\n",
    "    m.drawcountries()\n",
    "    m.drawcoastlines()\n",
    "    img_path = folder+str(int(timestamp))+'.png'\n",
    "    plt.savefig(img_path)\n",
    "    return img_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "image_paths = []\n",
    "for t in xrange(216, 236):\n",
    "\n",
    "    id_test, y_test, X_map_test = nn.select(t=t, subset=subset)\n",
    "    y_test = reformat_y(y_test)\n",
    "    X_map_test = (X_map_test-mean_train)/std_train\n",
    "    \n",
    "    timestamp = nn.times[(id_test[0,0]).astype(int)]\n",
    "    xs = nn.lons[(id_test[:,2]).astype(int)]\n",
    "    ys = nn.lats[(id_test[:,1]).astype(int)]\n",
    "    test_predict = model.predict(X_map_test)\n",
    "    \n",
    "    path = plot_compare_map(xs, ys, y_test[:,0], (test_predict[:,0]>threshold), timestamp)\n",
    "    image_paths.append(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for image_path in image_paths:\n",
    "    image = cv2.imread(image_path)\n",
    "    cv2.namedWindow(\"Image\", cv2.WINDOW_NORMAL)\n",
    "    pt = (0, 3 * image.shape[0] // 4)\n",
    "    cv2.imshow(\"Image\", image)\n",
    "    cv2.waitKey(3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # # graph model with two inputs and one output \n",
    "# # graph = Graph() \n",
    "# # graph.add_input(name='input1', input_shape=(32,)) \n",
    "# # graph.add_input(name='input2', input_shape=(32,)) \n",
    "# # graph.add_node(Dense(16), name='dense1', input='input1') \n",
    "# # graph.add_node(Dense(4), name='dense2', input='input2') \n",
    "# # graph.add_node(Dense(4), name='dense3', input='dense1') \n",
    "\n",
    "# # graph.add_output(name='output', inputs=['dense2', 'dense3'], merge_mode='sum') \n",
    "\n",
    "# # graph.compile(optimizer='rmsprop', loss={'output':'mse'}) \n",
    "# # history = graph.fit({'input1':X_train, 'input2':X2_train, 'output':y_train}, nb_epoch=10) \n",
    "# # predictions = graph.predict({'input1':X_test, 'input2':X2_test}) # {'output':...}\n",
    "\n",
    "\n",
    "# # graph model with two inputs and one output \n",
    "# model = Graph() \n",
    "\n",
    "# map_dimensions=train_X_map[0][0].shape\n",
    "\n",
    "# # two types of inputs: maps in 3D matrix and a list\n",
    "# model.add_input(name='maps', input_shape=train_X_map[0].shape) \n",
    "# model.add_input(name='lst', input_shape=train_X_lst[0].shape) \n",
    "\n",
    "# # adding layers to process the maps\n",
    "# # \n",
    "# model.add_node(Convolution2D(64, 3, 3, activation='relu', border_mode='same', dim_ordering='th')\n",
    "#                , name='map_conv1', input='maps')\n",
    "# # add pooling layers\n",
    "# model.add_node(Flatten(), name='map_flatten', input='map_conv1')\n",
    "# model.add_node(Dense(64), name='map_dense1', input='map_flatten')\n",
    "\n",
    "# # adding layers to process the lst\n",
    "# model.add_node(Dense(16), name='lst_dense1', input='lst') \n",
    "# model.add_node(Dense(8), name='lst_dense2', input='lst_dense1')\n",
    "\n",
    "# # merging two sets of weights\n",
    "# model.add_node(Dense(72, activation='relu'), name='combine', inputs=['map_flatten', 'lst_dense1'], merge_mode='concat')\n",
    "# model.add_node(Dense(1, activation='sigmoid'), name='reduce', input='combine')\n",
    "# model.add_output(name='output', input='reduce')\n",
    "\n",
    "# sgd = SGD(lr=0.001)\n",
    "# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy']) \n",
    "\n",
    "# # use dropouts \n",
    "# # try different solver\n",
    "# # if acc = 0, network diverged, reduce step size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# model.fit({'maps': train_X_map, 'lst': train_X_lst, 'output': train_y}, nb_epoch=10, batch_size=10, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# prediction = model.predict({'maps': np.array(test_X_map), 'lst': np.array(test_X_lst)}, batch_size=1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
