{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import utility libraries\n",
    "from netCDF4 import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.basemap import Basemap, cm\n",
    "%matplotlib inline\n",
    "\n",
    "# import machine learning tools\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Convolution2D, ZeroPadding2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "\n",
    "# import utilities and classes I wrote\n",
    "from clustering import Location_Clusterer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting nn_input.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile nn_input.py\n",
    "# import utility libraries\n",
    "from netCDF4 import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "\n",
    "# import machine learning tools\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Convolution2D, ZeroPadding2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "\n",
    "# import utilities and classes I wrote\n",
    "from clustering import Location_Clusterer\n",
    "\n",
    "class NN_Input(object):\n",
    "    \"\"\"\n",
    "    Stores the input data ready for feeding into a keras neural network. \n",
    "\n",
    "    To-Do:\n",
    "    - add function to take the clustering data in some ways\n",
    "    - add function to return the actual lat, lon, and time based on indices\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, predict=2, history=3):\n",
    "        \"\"\"\n",
    "        Initialize a class for storing neural network input data. \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        predict: int, number of time points ahead that the model will predict. \n",
    "                 For example, if predict=2, the model will predict 2 time points away from the given time. \n",
    "        history: int, number of time points for which data would be included as input.\n",
    "                 For example, if data_length=3, the model will receive 3 time points worth of data (current time\n",
    "                 point, the previous time point, and the timep point before that).\n",
    "        \"\"\"\n",
    "        self.lons = None\n",
    "        self.lats = None\n",
    "        self.times = None\n",
    "        \n",
    "        self.labels = None\n",
    "        self.features = OrderedDict()\n",
    "        self.feature_types = OrderedDict()\n",
    "        \n",
    "        self.predict = predict\n",
    "        self.history = history\n",
    "        \n",
    "    def load_labels(self, f_path, var):\n",
    "        \"\"\"\n",
    "        Load labels from netCDF file. \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        f_path: string\n",
    "        var: string\n",
    "        \"\"\"\n",
    "        nc = Dataset(f_path, 'r')\n",
    "        self.lons = nc.variables['lon'][:]\n",
    "        self.lats = nc.variables['lat'][:]\n",
    "        \n",
    "        self.times = nc.variables['time'][self.history-1:-self.predict]\n",
    "        n = self.predict + self.history - 1\n",
    "        self.labels = nc.variables[var][n:,:,:]\n",
    "        \n",
    "    def load_features(self, f_path, var, name, feature_type):\n",
    "        \"\"\"\n",
    "        Load feature values from netCDF files. Stores feature type information. \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        f_path: string, path to input netCDF file.\n",
    "        var: string, variable name as appeared in the netCDF file. \n",
    "        name: string, name of the variable to be stored. \n",
    "        feature_type: string, must be one of the following: 'history_time_series', 'forecast_time_series', \n",
    "        'multi_layers', 'single_layer'\n",
    "        \"\"\"\n",
    "        nc = Dataset(f_path, 'r')\n",
    "        temp_data = nc.variables[var][:]\n",
    "        \n",
    "        # Storing information on whether the input features \n",
    "        self.feature_types[name] = feature_type\n",
    "        \n",
    "        if self.feature_types[name] == 'history_time_series':\n",
    "            self.features[name] = temp_data[:-self.predict, :, :]\n",
    "        elif self.feature_types[name] == 'forecast_time_series':\n",
    "            self.features[name] = temp_data[self.history-1:, :, :]\n",
    "        else:\n",
    "            self.features[name] = temp_data\n",
    "        \n",
    "    \n",
    "    def get_features(self, i, j, k):\n",
    "        \"\"\"\n",
    "        Given indices for latitude, longitude, and time point, returns the associated data from self.data. \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        lat: int, index for the latitude desired. Must be within the range available in self.data. \n",
    "        lon: int, index for the longitude desired. Must be within the range available in self.data. \n",
    "        time: int, index for the time point desired. Must be within the range available in self.data. \n",
    "        \"\"\"\n",
    "        output = []\n",
    "        for feat in self.variables:\n",
    "            if self.feature_types[feat] == 'history_time_series':\n",
    "                pass\n",
    "            elif self.feature_types[feat] == 'forecast_time_series':\n",
    "                pass\n",
    "            elif self.feature_types[feat] == 'multi_layers':\n",
    "                pass\n",
    "            else: \n",
    "                pass\n",
    "        \n",
    "    def select(self, n, cutoff=None):\n",
    "        if cutoff is None:\n",
    "            cutoff = len(self.times)/2\n",
    "            \n",
    "        output = []\n",
    "        while len(output) < n:\n",
    "            i = np.random.randint(cutoff)\n",
    "            j = np.random.randint(self.lats)\n",
    "            k = np.random.randint(self.lons)\n",
    "            features, masks = self.get_features(i, j, k)\n",
    "            if not np.any(masks):\n",
    "                indices = (i, j, k)\n",
    "                label = self.labels[i, j, k]\n",
    "                output.append([indices, label, features])\n",
    "        return output\n",
    "    \n",
    "    def variables(self):\n",
    "        return self.features.keys()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nn_input import NN_Input\n",
    "\n",
    "folder = '/home/ubuntu/dataset/'\n",
    "\n",
    "nn = NN_Input(predict=2, history=3)\n",
    "nn.load_labels(folder+'sign.label.nc', 'Band1')\n",
    "\n",
    "f_paths = ['all.ndvi.nc','all.max.of.Wind.nc', 'all.min.of.Tmin.nc', 'all.mean.of.Tmin.nc', 'all.sum.of.Prec.nc',\n",
    "           'all.max.of.Tmax.nc', 'all.mean.of.Tmax.nc','elev.nc', 'veg.nc']\n",
    "variables = ['Band1', 'Wind', 'Tmin', 'Tmin', 'Prec', 'Tmax', 'Tmax', 'elev', 'Cv']\n",
    "names = ['ndvi', 'max_wind', 'min_tmin', 'mean_tmin', 'total_prec', 'max_tmax', 'mean_tmax', 'elev', 'veg']\n",
    "feature_types = ['history_time_series', 'forecast_time_series', 'forecast_time_series', 'forecast_time_series',\n",
    "                 'forecast_time_series', 'forecast_time_series', 'forecast_time_series',\n",
    "                'single_layer', 'multi_layer']\n",
    "\n",
    "for f_path, v, n, feature_type in zip(f_paths, variables, names, feature_types):\n",
    "    nn.load_features(folder+f_path, v, n, feature_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['min_tmin',\n",
       " 'veg',\n",
       " 'total_prec',\n",
       " 'mean_tmin',\n",
       " 'elev',\n",
       " 'ndvi',\n",
       " 'max_tmax',\n",
       " 'mean_tmax',\n",
       " 'max_wind']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.features.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['all.max.of.Wind.nc',\n",
       " 'all.min.of.Tmin.nc',\n",
       " 'all.mean.of.Tmin.nc',\n",
       " 'tmean.monthly.mask.nc',\n",
       " 'all.sum.of.Prec.nc',\n",
       " 'all.max.of.Tmax.nc',\n",
       " 'LDAS_veg_lib',\n",
       " 'mask.nc',\n",
       " 'all.ndvi.nc',\n",
       " 'veg.nc',\n",
       " 'all.mean.of.Tmax.nc',\n",
       " 'ppt.monthly.mask.nc',\n",
       " 'elev.nc',\n",
       " 'sign.label.nc']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir('/home/ubuntu/dataset/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_folder = '/Users/Chiao/google-drive/projects/Galvanize/fall-foliage-finder/data/'\n",
    "\n",
    "# Getting my labels\n",
    "nc = Dataset(data_folder+'sign.label.nc', 'r')\n",
    "labels = nc.variables['Band1'][4:][np.newaxis]\n",
    "lons = nc.variables['lon'][:]\n",
    "lats = nc.variables['lat'][:]\n",
    "times = nc.variables['time'][:]\n",
    "\n",
    "nc = Dataset(data_folder+'all.ndvi.nc')\n",
    "ndvis = nc.variables['Band1'][:]\n",
    "\n",
    "ndvis = np.ma.concatenate((ndvis[2:-2][np.newaxis], ndvis[1:-3][np.newaxis], ndvis[0:-4][np.newaxis]), axis=0)\n",
    "mask = ndvis.mask[:]\n",
    "\n",
    "ndvis = np.lib.pad(ndvis, ((0, 0), (0, 0), (5, 5), (5, 5)), 'constant', constant_values=(-3000))\n",
    "\n",
    "nobs = len(times)*len(lats)*len(lons)\n",
    "\n",
    "t0 = datetime.now()\n",
    "flatten_data = np.zeros((nobs, 3, 10,10))\n",
    "n = 0\n",
    "for i, t in enumerate(times):\n",
    "    for j, lat in enumerate(lats):\n",
    "        for k, lon in enumerate(lons):\n",
    "            flatten_data[n, :] = np.squeeze(ndvis[:, i, j:j+10, k:k+10])\n",
    "            n += 1\n",
    "print 'that took:', datetime.now()-t0, 'seconds'\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(437, 614, 927)\n",
      "(437, 614, 927)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "f_paths = ['all.ndvi.nc', 'sign.label.nc', 'elev.nc']\n",
    "names = ['ndvi', 'label', 'elev']\n",
    "\n",
    "\n",
    "# util function to open, resize and format pictures into appropriate tensors\n",
    "def preprocess_image(image_path):\n",
    "    img = imresize(imread(image_path), (img_width, img_height))\n",
    "    img = img.transpose((2, 0, 1)).astype('float64')\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    return img\n",
    "\n",
    "# util function to convert a tensor into a valid image\n",
    "def deprocess_image(x):\n",
    "    x = x.transpose((1, 2, 0))\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # subsetting the first 100 time points to work with\n",
    "# # 100 time points is around 2 years of data\n",
    "# n = 100\n",
    "\n",
    "# # flatten the 3D array into a 1D column\n",
    "# coords = np.meshgrid(times[:n], lats[10:-10], lons[10:-10], indexing='ij')\n",
    "# nobs = n*len(lats[10:-10])*len(lons[10:-10])\n",
    "# flatten_labels = np.zeros((nobs, len(coords)+2))\n",
    "\n",
    "# for i in xrange(len(coords)):\n",
    "#     flatten_labels[:,i] = coords[i].flatten()\n",
    "    \n",
    "# # get the label from 2 timestamps away \n",
    "# flatten_labels[:,-2] = labels[2:n+2, 10:-10, 10:-10].flatten()\n",
    "\n",
    "# # stack the mask together from current time point and the future time point\n",
    "# # get the composite\n",
    "# current_mask = labels.mask[:n, 10:-10, 10:-10].flatten()\n",
    "# future_mask = labels.mask[2:n+2, 10:-10, 10:-10].flatten()\n",
    "# flatten_labels[:,-1] = np.any(np.vstack((current_mask, future_mask)), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# base_image = data['ndvi'][:20]\n",
    "# result_prefix = 'result'\n",
    "\n",
    "# # dimensions of the generated picture.\n",
    "# img_width, img_height = data['label'].shape[-2:]\n",
    "\n",
    "\n",
    "\n",
    "# # build the VGG16 network\n",
    "# model = Sequential()\n",
    "\n",
    "# model.add(ZeroPadding2D((1, 1), batch_input_shape=(1, 3, img_width, img_height)))\n",
    "\n",
    "# model.add(Convolution2D(64, 3, 3, activation='relu', name='conv1_1'))\n",
    "# model.add(ZeroPadding2D((1, 1)))\n",
    "# model.add(Convolution2D(64, 3, 3, activation='relu', name='conv1_2'))\n",
    "# model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "# model.add(ZeroPadding2D((1, 1)))\n",
    "# model.add(Convolution2D(128, 3, 3, activation='relu', name='conv2_1'))\n",
    "# model.add(ZeroPadding2D((1, 1)))\n",
    "# model.add(Convolution2D(128, 3, 3, activation='relu', name='conv2_2'))\n",
    "# model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "\n",
    "# # get the symbolic outputs of each \"key\" layer (we gave them unique names).\n",
    "# layer_dict = dict([(layer.name, layer) for layer in model.layers])\n",
    "\n",
    "# # continuity loss util function\n",
    "# def continuity_loss(x):\n",
    "#     assert K.ndim(x) == 4\n",
    "#     a = K.square(x[:, :, :img_width-1, :img_height-1] - x[:, :, 1:, :img_height-1])\n",
    "#     b = K.square(x[:, :, :img_width-1, :img_height-1] - x[:, :, :img_width-1, 1:])\n",
    "#     return K.sum(K.pow(a + b, 1.25))\n",
    "\n",
    "# # define the loss\n",
    "# loss = K.variable(0.)\n",
    "# for layer_name in settings['features']:\n",
    "#     # add the L2 norm of the features of a layer to the loss\n",
    "#     assert layer_name in layer_dict.keys(), 'Layer ' + layer_name + ' not found in model.'\n",
    "#     coeff = settings['features'][layer_name]\n",
    "#     x = layer_dict[layer_name].output\n",
    "#     shape = layer_dict[layer_name].output_shape\n",
    "#     # we avoid border artifacts by only involving non-border pixels in the loss\n",
    "#     loss -= coeff * K.sum(K.square(x[:, :, 2: shape[2]-2, 2: shape[3]-2])) / np.prod(shape[1:])\n",
    "\n",
    "# # add continuity loss (gives image local coherence, can result in an artful blur)\n",
    "# loss += settings['continuity'] * continuity_loss(dream) / (3 * img_width * img_height)\n",
    "# # add image L2 norm to loss (prevents pixels from taking very high values, makes image darker)\n",
    "# loss += settings['dream_l2'] * K.sum(K.square(dream)) / (3 * img_width * img_height)\n",
    "\n",
    "# # feel free to further modify the loss as you see fit, to achieve new effects...\n",
    "\n",
    "# # compute the gradients of the dream wrt the loss\n",
    "# grads = K.gradients(loss, dream)\n",
    "\n",
    "# outputs = [loss]\n",
    "# if type(grads) in {list, tuple}:\n",
    "#     outputs += grads\n",
    "# else:\n",
    "#     outputs.append(grads)\n",
    "\n",
    "# f_outputs = K.function([dream], outputs)\n",
    "# def eval_loss_and_grads(x):\n",
    "#     x = x.reshape((1, 3, img_width, img_height))\n",
    "#     outs = f_outputs([x])\n",
    "#     loss_value = outs[0]\n",
    "#     if len(outs[1:]) == 1:\n",
    "#         grad_values = outs[1].flatten().astype('float64')\n",
    "#     else:\n",
    "#         grad_values = np.array(outs[1:]).flatten().astype('float64')\n",
    "#     return loss_value, grad_values\n",
    "\n",
    "# # this Evaluator class makes it possible\n",
    "# # to compute loss and gradients in one pass\n",
    "# # while retrieving them via two separate functions,\n",
    "# # \"loss\" and \"grads\". This is done because scipy.optimize\n",
    "# # requires separate functions for loss and gradients,\n",
    "# # but computing them separately would be inefficient.\n",
    "# class Evaluator(object):\n",
    "#     def __init__(self):\n",
    "#         self.loss_value = None\n",
    "#         self.grads_values = None\n",
    "\n",
    "#     def loss(self, x):\n",
    "#         assert self.loss_value is None\n",
    "#         loss_value, grad_values = eval_loss_and_grads(x)\n",
    "#         self.loss_value = loss_value\n",
    "#         self.grad_values = grad_values\n",
    "#         return self.loss_value\n",
    "\n",
    "#     def grads(self, x):\n",
    "#         assert self.loss_value is not None\n",
    "#         grad_values = np.copy(self.grad_values)\n",
    "#         self.loss_value = None\n",
    "#         self.grad_values = None\n",
    "#         return grad_values\n",
    "\n",
    "# evaluator = Evaluator()\n",
    "\n",
    "# # run scipy-based optimization (L-BFGS) over the pixels of the generated image\n",
    "# # so as to minimize the loss\n",
    "# x = preprocess_image(base_image_path)\n",
    "# for i in range(5):\n",
    "#     print('Start of iteration', i)\n",
    "#     start_time = time.time()\n",
    "\n",
    "#     # add a random jitter to the initial image. This will be reverted at decoding time\n",
    "#     random_jitter = (settings['jitter'] * 2) * (np.random.random((3, img_width, img_height)) - 0.5)\n",
    "#     x += random_jitter\n",
    "\n",
    "#     # run L-BFGS for 7 steps\n",
    "#     x, min_val, info = fmin_l_bfgs_b(evaluator.loss, x.flatten(),\n",
    "#                                      fprime=evaluator.grads, maxfun=7)\n",
    "#     print('Current loss value:', min_val)\n",
    "#     # decode the dream and save it\n",
    "#     x = x.reshape((3, img_width, img_height))\n",
    "#     x -= random_jitter\n",
    "#     img = deprocess_image(x)\n",
    "#     fname = result_prefix + '_at_iteration_%d.png' % i\n",
    "#     imsave(fname, img)\n",
    "#     end_time = time.time()\n",
    "#     print('Image saved as', fname)\n",
    "#     print('Iteration %d completed in %ds' % (i, end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
